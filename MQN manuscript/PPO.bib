Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop


@book{BermPlem1994,
	Author = {Berman, Abraham and Plemmons, Robert J.},
	Doi = {\href{https://doi.org/10.1137/1.9781611971262}{Crossref}},
	Isbn = {0-89871-321-8},
	Mrclass = {15A48 (15-02 60J10 90C33)},
	Mrnumber = {1298430},
	Note = {Revised reprint of the 1979 original},
	Pages = {xx+340},
	Publisher = {Society for Industrial and Applied Mathematics (SIAM), Philadelphia, PA},
	Series = {Classics in Applied Mathematics},
	Title = {Nonnegative matrices in the mathematical sciences},
	%Url = {https://doi.org/10.1137/1.9781611971262},
	Volume = {9},
	Year = {1994},
    Addendum =  {\href{https://mathscinet.ams.org/mathscinet-getitem?mr=MR1298430}{MR1298430}, \href{https://doi.org/10.1137/1.9781611971262}{Crossref}}
	%Bdsk-Url-1 = {https://doi.org/10.1137/1.9781611971262}
}


@inproceedings{Ramirez-Hernandez2009,
author = {Ramirez-Hernandez, Jose A. and Fernandez, Emmanuel},
booktitle = {Proceedings of the 2009 Winter Simulation Conference (WSC)},
doi = {10.1109/WSC.2009.5429179},
isbn = {978-1-4244-5770-0},
month = {dec},
pages = {1634--1645},
publisher = {IEEE},
title = {{A simulation-based Approximate Dynamic Programming approach for the control of the Intel Mini-Fab benchmark model}},
url = {http://ieeexplore.ieee.org/document/5429179/},
year = {2009}
}
@article{Niemiro2009,
author = {Niemiro, Wojciech and Pokarowski, Piotr},
journal = { Journal of Applied Probability},
number = {2},
pages = {309--329},
title = {{Fixed precision MCMC estimation by median of products of averages}},
volume = {46},
year = {2009}
}
@inproceedings{Kakade2002,
author = {Kakade, Sham and Langford, John},
booktitle = {Proceedings of the Nineteenth International Conference on Machine Learning},
isbn = {1558608737},
pages = {267--274},
publisher = {Morgan Kaufmann Publishers},
title = {{Approximately Optimal Approximate Reinforcement Learning}},
year = {2002}
}
@article{Kumar1993a,
author = {Kumar, P. R.},
doi = {10.1007/BF01158930},
issn = {0257-0130},
journal = {Queueing Systems},
month = {mar},
number = {1-3},
pages = {87--110},
publisher = {Kluwer Academic Publishers},
title = {{Re-entrant lines}},
url = {http://link.springer.com/10.1007/BF01158930},
volume = {13},
year = {1993}
}
@article{Dai1996,
abstract = {Reentrant lines can be used to model complex manufacturing systems such as wafer fabrication facilities. As the first step to the optimal or near-optimal scheduling of such lines, we investigate th...},
author = {Dai, J. G. and Weiss, G.},
doi = {10.1287/moor.21.1.115},
issn = {0364-765X},
journal = {Mathematics of Operations Research},
keywords = {Harris recurrence,fluid models,multiclass queueing networks,piecewise linear Lyapunoc functions,reentrant lines,scheduling policies,stability,unstable networks},
month = {feb},
number = {1},
pages = {115--134},
publisher = { INFORMS },
title = {{Stability and Instability of Fluid Models for Reentrant Lines}},
url = {http://pubsonline.informs.org/doi/abs/10.1287/moor.21.1.115},
volume = {21},
year = {1996}
}
@techreport{Harrison1998,
abstract = {This paper is concerned with dynamic scheduling in a queueing system that has two independent Poisson input streams, two servers, deterministic service times and linear holding costs. One server can process both classes of incoming jobs, but the other can process only one class, and the service time for the shared job class is different depending on which server is involved. A bound on system performance is developed in terms of a single pooled resource, or super-server, whose capabilities combine those of the original two servers. Thereafter, attention is focused on the heavy traffic regime, where the combined capacity of the two servers is approximately equal to the total input rate. We construct a discrete-review control policy and show that if its parameters are chosen correctly as one approaches the heavy traffic limit, then its cost performance approaches the bound associated with a single pooled resource. Thus the discrete-review policy is proved to be asymptotically optimal in the heavy traffic limit. Although resource pooling in heavy traffic has been observed to occur in other network scheduling problems, there have been very few studies that rigorously proved the pooling phenomenon, or that proved the asymptotic optimality of a specific policy. Our discrete-review policy is obtained by applying a general method, called the BIGSTEP method in an earlier paper, to the parallel-server model.},
author = {Harrison, J Michael},
booktitle = {The Annals of Applied Probability},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Harrison - 1998 - HEAVY TRAFFIC ANALYSIS OF A SYSTEM WITH PARALLEL SERVERS ASYMPTOTIC OPTIMALITY OF DISCRETE-REVIEW POLICIES.pdf:pdf},
number = {3},
pages = {822--848},
title = {{HEAVY TRAFFIC ANALYSIS OF A SYSTEM WITH PARALLEL SERVERS: ASYMPTOTIC OPTIMALITY OF DISCRETE-REVIEW POLICIES}},
url = {https://projecteuclid.org/download/pdf{\_}1/euclid.aoap/1028903452},
volume = {8},
year = {1998}
}
@article{Salimans2018,
abstract = {We propose a new method for learning from a single demonstration to solve hard exploration tasks like the Atari game Montezuma's Revenge. Instead of imitating human demonstrations, as proposed in other recent works, our approach is to maximize rewards directly. Our agent is trained using off-the-shelf reinforcement learning, but starts every episode by resetting to a state from a demonstration. By starting from such demonstration states, the agent requires much less exploration to learn a game compared to when it starts from the beginning of the game at every episode. We analyze reinforcement learning for tasks with sparse rewards in a simple toy environment, where we show that the run-time of standard RL methods scales exponentially in the number of states between rewards. Our method reduces this to quadratic scaling, opening up many tasks that were previously infeasible. We then apply our method to Montezuma's Revenge, for which we present a trained agent achieving a high-score of 74,500, better than any previously published result.},
archivePrefix = {arXiv},
arxivId = {1812.03381},
author = {Salimans, Tim and Chen, Richard},
eprint = {1812.03381},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Salimans, Chen - 2018 - Learning Montezuma's Revenge from a Single Demonstration.pdf:pdf},
month = {dec},
title = {{Learning Montezuma's Revenge from a Single Demonstration}},
url = {http://arxiv.org/abs/1812.03381},
year = {2018}
}
@article{Kartashov1985,
abstract = {In this paper, we are interested in investigating the perturbation bounds for the stationary distributions for discrete-time or continuous-time Markov chains on a countable state space. For discrete-time Markov chains, two new norm-wise bounds are obtained. The first bound is rather easy to be obtained since the needed condition, equivalent to uniform ergodicity, is imposed on the transition matrix directly. The second bound, which holds for a general (possibly periodic) Markov chain, involves finding a drift function. This drift function is closely related with the mean first hitting times. Some V-norm-wise bounds are also derived based on the results in [11]. Moreover, we show how the bounds developed in this paper and one bound given in [24] can be extended to continuous-time Markov chains. Several examples are shown to illustrate our results or to compare our bounds with the known ones in the literature.},
archivePrefix = {arXiv},
arxivId = {1208.4974v1},
author = {Kartashov, N.},
eprint = {1208.4974v1},
journal = {Theory Probab. Appl.},
pages = {71--89},
title = {{Criteria for uniform ergodicity and strong stability of Markov chains with a common phase space}},
url = {https://arxiv.org/pdf/1208.4974.pdf},
volume = {30},
year = {1985}
}
@article{Schulman2017,
abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
archivePrefix = {arXiv},
arxivId = {1707.06347},
author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
eprint = {1707.06347},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:pdf},
month = {jul},
title = {{Proximal Policy Optimization Algorithms}},
url = {http://arxiv.org/abs/1707.06347},
year = {2017}
}
@article{Bauerle2002,
author = {B{\"{a}}uerle, Nicole},
journal = { Advances in Applied Probability},
number = {2},
pages = {313--328},
title = {{Optimal Control of Queueing Networks: An Approach via Fluid Models}},
url = {http://link.springer.com/10.1007/BF01149168},
volume = {34},
year = {2002}
}
@article{Fill1991,
author = {Fill, James Allen},
doi = {10.1214/aoap/1177005981},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fill - 1991 - Eigenvalue Bounds on Convergence to Stationarity for Nonreversible Markov Chains, with an Application to the Exclusion Pro.pdf:pdf},
issn = {1050-5164},
journal = {The Annals of Applied Probability},
keywords = {Cheege's inequality,Markov chains,Poincare inequality,Poisson blockers,chi-square distance,exclusion process,interacting particle systems,rapid mixing,rates of convergence,reversibility,variation distance},
month = {feb},
number = {1},
pages = {62--87},
publisher = {Institute of Mathematical Statistics},
title = {{Eigenvalue Bounds on Convergence to Stationarity for Nonreversible Markov Chains, with an Application to the Exclusion Process}},
url = {http://projecteuclid.org/euclid.aoap/1177005981},
volume = {1},
year = {1991}
}
@techreport{Henderson2005,
abstract = {We use simulation to estimate the steady-state performance of a stable multiclass queueing network. Standard estimators have been seen to perform poorly when the network is heavily loaded. We introduce two new simulation estimators. The first provides substantial variance reductions in moderately-loaded networks at very little additional computational cost. The second estimator provides substantial variance reductions in heavy traffic, again for a small additional computational cost. Both methods employ the variance reduction method of control variates, and differ in terms of how the control variates are constructed.},
author = {Henderson, Shane G and Meyn, Sean P},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Henderson, Meyn - 2005 - Variance Reduction in Simulation of Multiclass Processing Networks.pdf:pdf},
title = {{Variance Reduction in Simulation of Multiclass Processing Networks}},
year = {2005}
}
@article{Williams1998,
author = {Williams, R.J.},
doi = {10.1023/A:1019108819713},
issn = {02570130},
journal = {Queueing Systems},
number = {1/2},
pages = {27--88},
publisher = {Kluwer Academic Publishers},
title = {{Diffusion approximations for open multiclass queueing networks: sufficient conditions involving state space collapse}},
url = {http://link.springer.com/10.1023/A:1019108819713},
volume = {30},
year = {1998}
}
@inproceedings{Marbach,
author = {Marbach, P. and Tsitsiklis, J.N.},
booktitle = {Proceedings of the 37th IEEE Conference on Decision and Control (Cat. No.98CH36171)},
doi = {10.1109/CDC.1998.757861},
isbn = {0-7803-4394-8},
pages = {2698--2703},
publisher = {IEEE},
title = {{Simulation-based optimization of Markov reward processes}},
url = {http://ieeexplore.ieee.org/document/757861/},
volume = {3},
year = {1998}
}
@inproceedings{Schulman2016,
author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael I. and Abbeel, Pieter},
booktitle = {ICLR},
title = {{High-Dimensional Continuous Control Using Generalized Advantage Estimation}},
url = {https://www.semanticscholar.org/paper/High-Dimensional-Continuous-Control-Using-Advantage-Schulman-Moritz/1aa02f027663a626f3aa17ed9bab52377a23f634},
year = {2016}
}
@article{Haarnoja2018a,
author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
journal = {Proceedings of Machine Learning Research},
pages = {1861--1870},
title = {{Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor}},
url = {http://proceedings.mlr.press/v80/haarnoja18b.html},
volume = {80},
year = {2018}
}
@article{Adan2018,
abstract = {We study a parallel queueing system with multiple types of servers and customers. A bipartite graph describes which pairs of customer-server types are compatible. We consider the service policy that always assigns servers to the first, longest waiting compatible customer, and that always assigns customers to the longest idle compatible server if on arrival multiple compatible servers are available. For a general renewal stream of arriving customers, general service time distributions that depend both on customer and on server types, and general customer patience distributions, the behavior of such systems is very complicated. Key quantities for their performance are the matching rates, the fraction of services for each pair of compatible customer-server. Calculation of these matching rates in general is intractable, it depends on the entire shape of service time distributions. We suggest through a heuristic argument that if the number of servers becomes large, the matching rates are well approximated by matching rates calculated from the tractable bipartite infinite matching model. We present simulation evidence to support this heuristic argument, and show how this can be used to design systems with desired performance requirements.},
author = {Adan, Ivo J B F and Boon, Marko A A and Weiss, Gideon},
doi = {10.1016/j.ejor.2018.08.042},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Adan, Boon, Weiss - 2018 - Stochastics and Statistics Design heuristic for parallel many server systems R.pdf:pdf},
journal = {European Journal of Operational Research},
keywords = {Matching rates,Multi-type customers and servers,Parallel service systems,Queueing,Resource pooling},
pages = {259--277},
title = {{Stochastics and Statistics Design heuristic for parallel many server systems R}},
url = {https://doi.org/10.1016/j.ejor.2018.08.042},
volume = {273},
year = {2018}
}
@inproceedings{Abadi2016,
author = {Abadi, Mart{\'{i}}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
booktitle = {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)},
isbn = {9781931971331},
pages = {265----283},
publisher = {USENIX Association},
title = {{TensorFlow: A System for Large-Scale Machine Learning}},
url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi},
year = {2016}
}
@inproceedings{Ramirez-Hernandez2007c,
author = {Ramirez-Hernandez, Jose A. and Fernandez, Emmanuel},
booktitle = {Sixth International Conference on Machine Learning and Applications (ICMLA 2007)},
doi = {10.1109/ICMLA.2007.78},
isbn = {978-0-7695-3069-7},
month = {dec},
pages = {330--335},
publisher = {IEEE},
title = {{Control of a re-entrant line manufacturing model with a reinforcement learning approach}},
url = {http://ieeexplore.ieee.org/document/4457252/},
year = {2007}
}
@book{Bertsekas2012a,
abstract = {Fourth edition. v. 1. [no special title] -- v. 2. Approximate dynamic programming.},
address = {Belmont},
author = {Bertsekas, Dimitri P.},
edition = {4th},
isbn = {1886529434},
pages = {712},
publisher = {Athena Scientific},
title = {{Dynamic programming and optimal control V.II}},
year = {2012}
}
@article{Mnih2016,
abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
archivePrefix = {arXiv},
arxivId = {1602.01783},
author = {Mnih, Volodymyr and Badia, Adri{\`{a}} Puigdom{\`{e}}nech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
eprint = {1602.01783},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learning.pdf:pdf},
month = {feb},
title = {{Asynchronous Methods for Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1602.01783},
year = {2016}
}
@article{Machado2017,
abstract = {The Arcade Learning Environment (ALE) is an evaluation platform that poses the challenge of building AI agents with general competency across dozens of Atari 2600 games. It supports a variety of different problem settings and it has been receiving increasing attention from the scientific community, leading to some high-profile success stories such as the much publicized Deep Q-Networks (DQN). In this article we take a big picture look at how the ALE is being used by the research community. We show how diverse the evaluation methodologies in the ALE have become with time, and highlight some key concerns when evaluating agents in the ALE. We use this discussion to present some methodological best practices and provide new benchmark results using these best practices. To further the progress in the field, we introduce a new version of the ALE that supports multiple game modes and provides a form of stochasticity we call sticky actions. We conclude this big picture look by revisiting challenges posed when the ALE was introduced, summarizing the state-of-the-art in various problems and highlighting problems that remain open.},
archivePrefix = {arXiv},
arxivId = {1709.06009},
author = {Machado, Marlos C. and Bellemare, Marc G. and Talvitie, Erik and Veness, Joel and Hausknecht, Matthew and Bowling, Michael},
eprint = {1709.06009},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Machado et al. - 2017 - Revisiting the Arcade Learning Environment Evaluation Protocols and Open Problems for General Agents.pdf:pdf},
month = {sep},
title = {{Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents}},
url = {http://arxiv.org/abs/1709.06009},
year = {2017}
}
@article{Komorowski2018,
abstract = {Sepsis is the third leading cause of death worldwide and the main cause of mortality in hospitals1‚Ä?, but the best treatment strategy remains uncertain. In particular, evidence suggests that current practices in the administration of intravenous fluids and vasopressors are suboptimal and likely induce harm in a proportion of patients1,4‚Ä?. To tackle this sequential decision-making problem, we developed a reinforcement learning agent, the Artificial Intelligence (AI) Clinician, which extracted implicit knowledge from an amount of patient data that exceeds by many-fold the life-time experience of human clinicians and learned optimal treatment by analyzing a myriad of (mostly suboptimal) treatment decisions. We demonstrate that the value of the AI Clinician's selected treatment is on average reliably higher than human clinicians. In a large validation cohort independent of the training data, mortality was lowest in patients for whom clinicians' actual doses matched the AI decisions. Our model provides individualized and clinically interpretable treatment decisions for sepsis that could improve patient outcomes.},
author = {Komorowski, Matthieu and Celi, Leo A. and Badawi, Omar and Gordon, Anthony C. and Faisal, A. Aldo},
doi = {10.1038/s41591-018-0213-5},
issn = {1078-8956},
journal = {Nature Medicine},
keywords = {Biomedical engineering,Computational models,Machine learning,Outcomes research},
month = {nov},
number = {11},
pages = {1716--1720},
publisher = {Nature Publishing Group},
title = {{The Artificial Intelligence Clinician learns optimal treatment strategies for sepsis in intensive care}},
url = {http://www.nature.com/articles/s41591-018-0213-5},
volume = {24},
year = {2018}
}
@article{Paschalidis2004,
author = {Paschalidis, I.C. and Su, C. and Caramanis, M.C.},
doi = {10.1109/TAC.2004.835389},
issn = {0018-9286},
journal = {IEEE Transactions on Automatic Control},
month = {oct},
number = {10},
pages = {1709--1722},
title = {{Target-Pursuing Scheduling and Routing Policies for Multiclass Queueing Networks}},
url = {http://ieeexplore.ieee.org/document/1341564/},
volume = {49},
year = {2004}
}
@article{Zahavy2016,
abstract = {In recent years there is a growing interest in using deep representations for reinforcement learning. In this paper, we present a methodology and tools to analyze Deep Q-networks (DQNs) in a non-blind matter. Moreover, we propose a new model, the Semi Aggregated Markov Decision Process (SAMDP), and an algorithm that learns it automatically. The SAMDP model allows us to identify spatio-temporal abstractions directly from features and may be used as a sub-goal detector in future work. Using our tools we reveal that the features learned by DQNs aggregate the state space in a hierarchical fashion, explaining its success. Moreover, we are able to understand and describe the policies learned by DQNs for three different Atari2600 games and suggest ways to interpret, debug and optimize deep neural networks in reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1602.02658},
author = {Zahavy, Tom and Zrihem, Nir Ben and Mannor, Shie},
eprint = {1602.02658},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zahavy, Zrihem, Mannor - 2016 - Graying the black box Understanding DQNs.pdf:pdf},
month = {feb},
title = {{Graying the black box: Understanding DQNs}},
url = {http://arxiv.org/abs/1602.02658},
year = {2016}
}
@article{Haarnoja2018,
abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
archivePrefix = {arXiv},
arxivId = {1801.01290},
author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
eprint = {1801.01290},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Haarnoja et al. - 2018 - Soft Actor-Critic Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.pdf:pdf},
month = {jan},
title = {{Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor}},
url = {http://arxiv.org/abs/1801.01290},
year = {2018}
}
@article{Kumar2001,
author = {Kumar, S. and Kumar, P.R.},
doi = {10.1109/70.964657},
issn = {1042296X},
journal = {IEEE Transactions on Robotics and Automation},
number = {5},
pages = {548--561},
title = {{Queueing network models in the design and analysis of semiconductor wafer fabs}},
url = {http://ieeexplore.ieee.org/document/964657/},
volume = {17},
year = {2001}
}
@incollection{Kakade2001,
author = {Kakade, Sham},
doi = {10.1007/3-540-44581-1_40},
pages = {605--615},
publisher = {Springer, Berlin, Heidelberg},
title = {{Optimizing Average Reward Using Discounted Rewards}},
url = {http://link.springer.com/10.1007/3-540-44581-1{\_}40},
year = {2001}
}
@article{Nichol2018,
abstract = {In this report, we present a new reinforcement learning (RL) benchmark based on the Sonic the Hedgehog (TM) video game franchise. This benchmark is intended to measure the performance of transfer learning and few-shot learning algorithms in the RL domain. We also present and evaluate some baseline algorithms on the new benchmark.},
archivePrefix = {arXiv},
arxivId = {1804.03720},
author = {Nichol, Alex and Pfau, Vicki and Hesse, Christopher and Klimov, Oleg and Schulman, John},
eprint = {1804.03720},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nichol et al. - 2018 - Gotta Learn Fast A New Benchmark for Generalization in RL(2).pdf:pdf},
month = {apr},
title = {{Gotta Learn Fast: A New Benchmark for Generalization in RL}},
url = {http://arxiv.org/abs/1804.03720},
year = {2018}
}
@techreport{Hosu,
abstract = {This paper introduces a novel method for learning how to play the most difficult Atari 2600 games from the Arcade Learning Environment using deep reinforcement learning. The proposed method, called human checkpoint replay, consists in using checkpoints sampled from human gameplay as starting points for the learning process. This is meant to compensate for the difficulties of current exploration strategies, such as $\epsilon$-greedy, to find successful control policies in games with sparse rewards. Like other deep reinforcement learning architectures, our model uses a convolutional neural network that receives only raw pixel inputs to estimate the state value function. We tested our method on Montezuma's Revenge and Private Eye, two of the most challenging games from the Atari platform. The results we obtained show a substantial improvement compared to previous learning approaches, as well as over a random player. We also propose a method for training deep reinforcement learning agents using human gameplay experience, which we call human experience replay.},
archivePrefix = {arXiv},
arxivId = {1607.05077v1},
author = {Hosu, Ionel-Alexandru and Rebedea, Traian},
eprint = {1607.05077v1},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hosu, Rebedea - Unknown - Playing Atari Games with Deep Reinforcement Learning and Human Checkpoint Replay.pdf:pdf},
title = {{Playing Atari Games with Deep Reinforcement Learning and Human Checkpoint Replay}},
url = {https://arxiv.org/pdf/1607.05077.pdf},
year = {2016}
}
@book{Dunkin2009,
author = {Dunkin, Ann. and {Association for Computing Machinery}, Emmanuel},
booktitle = {Winter Simulation Conference},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dunkin, Association for Computing Machinery - 2009 - Winter Simulation Conference.pdf:pdf},
isbn = {9781424457717},
pages = {1634--1645},
publisher = {Winter Simulation Conference},
title = {{Winter Simulation Conference.}},
url = {https://dl.acm.org/citation.cfm?id=1995678},
year = {2009}
}
@phdthesis{Marbach1998,
abstract = {Thesis (Ph. D.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 1998.},
author = {Marbach, Peter},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Marbach - 1998 - Simulation-based optimization of Markov decision processes.pdf:pdf},
keywords = {Electrical Engineering and Computer Science,Thesis},
pages = {169},
publisher = {Massachusetts Institute of Technology},
school = {Massachusetts Institute of Technology},
title = {{Simulation-based optimization of Markov decision processes}},
year = {1998}
}
@inproceedings{Chen2014a,
abstract = {In this paper, we investigate the control of non-conventional queueing networks, where multiple concurrent state transitions following non-exponential and general sojourn distributions are allowed. Two approximation schemes are discussed that produce an approximated Markovian model. We further propose to model the problem as an uncertain Markov decision processes (MDP) by considering the induced approximation error. A new simulation-based approach is investigated here, to give an overall optimal policy beyond the classic approach to such problems, e.g., a robust formulation. In particular, we approach the problem as one of finding a best overall control policy, via exploration and exploitation within a heuristic policy set. An approximate dynamic programming algorithm is then used, in connection with a parametric cost function, for efficiently learning and finding policies in this set. We show that the problem of finding the best control policy within this new policy set can be understood, equivalently, as one of finding the best set of parameters for one-stage cost function of the problem. Later, an integrated framework, denoted as extended actor-critic, is proposed to give a comprehensive treatment for those types of problems. Results of a case study are also presented and discussed.},
author = {Chen, Xiaoting and Fernandez, Emmanuel},
booktitle = {Proceedings of the 2014 Industrial and Systems Engineering Research Conference},
title = {{Control of Non-Conventional Queueing Networks: A Parametric and Approximate Dynamic Programming Approach}},
url = {https://pdfs.semanticscholar.org/7960/9c36c3a8db00f8f72832cd269a160785461e.pdf},
year = {2014}
}
@article{Abbasi_Yadkori2014,
abstract = {To reveal the antiangiogenic capability of cancer chemotherapy, we developed an alternative antiangiogenic schedule for administration of cyclophosphamide. We show here that this antiangiogenic schedule avoided drug resistance and eradicated Lewis lung carcinoma and L1210 leukemia, an outcome not possible with the conventional schedule. When Lewis lung carcinoma and EMT-6 breast cancer were made drug resistant before therapy, the antiangiogenic schedule suppressed tumor growth 3-fold more effectively than the conventional schedule. When another angiogenesis inhibitor, TNP-470, was added to the antiangiogenic schedule of cyclophosphamide, drug-resistant Lewis lung carcinomas were eradicated. Each dose of the antiangiogenic schedule of cyclophosphamide induced the apoptosis of endothelial cells within tumors, and endothelial cell apoptosis preceded the apoptosis of drug-resistant tumor cells. This antiangiogenic effect was more pronounced in p53-null mice in which the apoptosis of p53-null endothelial cells induced by cyclophosphamide was so vigorous that drug-resistant tumors comprising 4.5{\%} of body weight were eradicated. Thus, by using a dosing schedule of cyclophosphamide that provided more sustained apoptosis of endothelial cells within the vascular bed of a tumor, we show that a chemotherapeutic agent can more effectively control tumor growth in mice, regardless of whether the tumor cells are drug resistant.},
author = {Browder, Timothy and Butterfield, Catherine E. and Kr{\"{a}}ling, Birgit M. and Shi, Bin and Marshall, Blair and O'Reilly, Michael S. and Folkman, Judah},
issn = {00085472},
journal = {Cancer Research},
number = {7},
pages = {1878--1886},
title = {{Antiangiogenic scheduling of chemotherapy improves efficacy against experimental drug-resistant cancer}},
url = {https://pdf.sciencedirectassets.com/280252/1-s2.0-S1876735414X00024/1-s2.0-S1876735414000233/main.pdf?x-amz-security-token=AgoJb3JpZ2luX2VjEFkaCXVzLWVhc3QtMSJGMEQCIHqKzXvdF9ZYkh0GtA6iGtMnTQfPbmNOJg3mUiyTOUHsAiByprtXypGwMqNPy9VtGiSKsZ8bmJxpuJ{\%}2BdASqirxGsbC},
volume = {60},
year = {2000}
}
@article{Lehnert2018,
abstract = {In Reinforcement Learning, an intelligent agent has to make a sequence of decisions to accomplish a goal. If this sequence is long, then the agent has to plan over a long horizon. While learning the optimal policy and its value function is a well studied problem in Reinforcement Learning, this paper focuses on the structure of the optimal value function and how hard it is to represent the optimal value function. We show that the generalized Rademacher complexity of the hypothesis space of all optimal value functions is dependent on the planning horizon and independent of the state and action space size. Further, we present bounds on the action-gaps of action value functions and show that they can collapse if a long planning horizon is used. The theoretical results are verified empirically on randomly generated MDPs and on a grid-world fruit collection task using deep value function approximation. Our theoretical results highlight a connection between value function approximation and the Options framework and suggest that value functions should be decomposed along bottlenecks of the MDP's transition dynamics.},
author = {Lehnert, Lucas and Laroche, Romain and van Seijen, Harm},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lehnert, Laroche, Seijen - 2018 - On Value Function Representation of Long Horizon Problems.pdf:pdf},
journal = {Thirty-Second AAAI Conference on Artificial Intelligence},
keywords = {Reinforcement Learning},
month = {apr},
title = {{On Value Function Representation of Long Horizon Problems}},
url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16388},
year = {2018}
}
@inproceedings{Ramirez-Hernandez2007a,
author = {Ramirez-Hernandez, Jose A. and Fernandez, Emmanuel},
booktitle = {2007 IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning},
doi = {10.1109/ADPRL.2007.368189},
isbn = {1-4244-0706-0},
month = {apr},
pages = {201--208},
publisher = {IEEE},
title = {{An Approximate Dynamic Programming Approach for Job Releasing and Sequencing in a Reentrant Manufacturing Line}},
url = {https://ieeexplore.ieee.org/document/4220834/},
year = {2007}
}
@article{Henderson2002,
abstract = {?Knowledge of either analytical or numerical approximations should enable more efficient simulation estimators to be constructed.? This principle seems intuitively plausible and certainly attractive, yet no completely satisfactory general methodology has been developed to exploit it. The authors present a new approach for obtaining variance reduction in Markov process simulation that is applicable to a vast array of different performance measures. The approach relies on the construction of a martingale that is then used as an internal control variate.},
author = {Henderson, Shane G. and Glynn, Peter W.},
doi = {10.1287/moor.27.2.253.329},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Henderson, Glynn - 2002 - Approximating martingales for variance reduction in Markov process simulation.pdf:pdf},
issn = {0364765X},
journal = {Mathematics of Operations Research},
keywords = {Markov process,Martingale,Simulation,Variance reduction},
number = {2},
pages = {253--271},
publisher = {INFORMS Inst.for Operations Res.and the Management Sciences},
title = {{Approximating martingales for variance reduction in Markov process simulation}},
volume = {27},
year = {2002}
}
@book{Puterman1994,
abstract = {"A Wiley-Interscience publication." Markov Decision Processes focuses primarily on infinite horizon discrete time models and models with discrete time spaces while also examining models with arbitrary state spaces, finite horizon models, and continuous-time discrete state models. The book is organized around optimality criteria, using a common framework centered on the optimality (Bellman) equation for presenting results. The results are presented in a "theorem-proof" format and elaborated on through both discussion and examples, including results that are not available in any other book. A two-state Markov decision process model, presented in Chapter 3, is analyzed repeatedly throughout the book and demonstrates many results and algorithms. Markov Decision Processes covers recent research advances in such areas as countable state space models with average reward criterion, constrained models, and models with risk sensitive optimality criteria. It also explores several topics that have received little or no attention in other books, including modified policy iteration, multichain models with average reward criterion, and sensitive optimality. In addition, a Bibliographic Remarks section in each chapter comments on relevant historical references in the book's extensive, up-to-date bibliography ... numerous figures illustrate examples, algorithms, results, and computations ... a biographical sketch highlights the life and work of A.A. Markov ... an afterword discusses partially observed models and other key topics ... and appendices examine Markov chains, normed linear spaces, semi-continuous functions, and linear programming. Markov Decision Processes will prove to be invaluable to researchers in operations research, management science, and control theory. Its applied emphasis will serve the needs of researchers in communications and control engineering, economics, statistics, mathematics, computer science, and mathematical ecology. Moreover, its conceptual development from simple to complex models, numerous applications in text and problems, and background coverage of relevant mathematics will make it a highly useful textbook in courses on dynamic programming and stochastic control. The past decade has seen considerable theoretical and applied research on Markov decision processes, as well as the growing use of these models in ecology, economics, communications engineering, and other fields where outcomes are uncertain and sequential decision-making processes are needed. A timely response to this increased activity, Martin L. Puterman's new work provides a uniquely up-to-date, unified, and rigorous treatment of the theoretical, computational, and applied research on Markov decision process models. It discusses all major research directions in the field, highlights many significant applications of Markov decision processes models, and explores numerous important topics that have previously been neglected or given cursory coverage in the literature. 1. Introduction -- 2. Model Formulation -- 3. Examples -- 4. Finite-Horizon Markov Decision Processes -- 5. Infinite-Horizon Models: Foundations -- 6. Discounted Markov Decision Problems -- 7. The Expected Total-Reward Criterion -- 8. Average Reward and Related Criteria -- 9. The Average Reward Criterion-Multichain and Communicating Models -- 10. Sensitive Discount Optimality -- 11. Continuous-Time Models -- Appendix A. Markov Chains -- Appendix B. Semicontinuous Functions -- Appendix C. Normed Linear Spaces -- Appendix D. Linear Programming.},
author = {Puterman, Martin L.},
isbn = {0471619779},
pages = {649},
publisher = {Wiley},
title = {{Markov decision processes : discrete stochastic dynamic programming}},
url = {https://dl.acm.org/citation.cfm?id=528623},
year = {1994}
}
@techreport{Stampa,
abstract = {In this paper we design and evaluate a Deep-Reinforcement Learning agent that optimizes routing. Our agent adapts automatically to current traffic conditions and proposes tailored configurations that attempt to minimize the network delay. Experiments show very promising performance. Moreover, this approach provides important operational advantages with respect to traditional optimization algorithms.},
archivePrefix = {arXiv},
arxivId = {1709.07080v1},
author = {Stampa, Giorgio and Arias, Marta and S{\'{a}}nchez-Charles, David and Munt{\'{e}}s-Mulero, Victor and Cabellos, Albert},
eprint = {1709.07080v1},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Stampa et al. - Unknown - A Deep-Reinforcement Learning Approach for Software-Defined Networking Routing Optimization.pdf:pdf},
isbn = {1581137354},
keywords = {Deep Reinforcement Learning,Knowledge-Defined Networking,Routing optimization,SDN,Traffic Engineering},
title = {{A Deep-Reinforcement Learning Approach for Software-Defined Networking Routing Optimization}},
url = {http://knowledgedefinednetworking.org.}
}
@article{Dai1995,
author = {Dai, J. G.},
doi = {10.1214/aoap/1177004828},
issn = {1050-5164},
journal = {The Annals of Applied Probability},
keywords = {Harris positive recurrent,Multiclass queueing networks,fluid approximation,stability},
month = {feb},
number = {1},
pages = {49--77},
publisher = {Institute of Mathematical Statistics},
title = {{On Positive Harris Recurrence of Multiclass Queueing Networks: A Unified Approach Via Fluid Limit Models}},
url = {http://projecteuclid.org/euclid.aoap/1177004828},
volume = {5},
year = {1995}
}
@inproceedings{Kakade2001a,
abstract = {How Many Queries Are Needed to Learn One Bit of Information? / Hans Ulrich Simon -- Radial Basis Function Neural Networks Have Superlinear VC Dimension / Michael Schmitt -- Tracking a Small Set of Experts by Mixing Past Posteriors / Oliver Bousquet and Manfred K. Warmuth -- Potential-Based Algorithms in On-Line Prediction and Game Theory / Nicolo Cesa-Bianchi and Gabor Lugosi -- A Sequential Approximation Bound for Some Sample-Dependent Convex Optimization Problems with Applications in Learning / Tong Zhang -- Efficiently Approximating Weighted Sums with Exponentially Many Terms / Deepak Chawla, Lin Li and Stephen Scott -- Ultraconservative Online Algorithms for Multiclass Problems / Koby Crammer and Yoram Singer -- Estimating a Boolean Perceptron from Its Average Satisfying Assignment: A Bound on the Precision Required / Paul W. Goldberg -- Adaptive Strategies and Regret Minimization in Arbitrarily Varying Markov Environments / Shie Mannor and Nahum Shimkin -- Robust Learning -- Rich and Poor / John Case, Sanjay Jain and Frank Stephan / [and others] -- On the Synthesis of Strategies Identifying Recursive Functions / Sandra Zilles -- Intrinsic Complexity of Learning Geometrical Concepts from Positive Data / Sanjay Jain and Efim Kinber -- Toward a Computational Theory of Data Acquisition and Truthing / David G. Stork -- Discrete Prediction Games with Arbitrary Feedback and Loss / Antonio Piccolboni and Christian Schindelhauer -- Rademacher and Gaussian Complexities: Risk Bounds and Structural Results / Peter L. Bartlett and Shahar Mendelson -- Further Explanation of the Effectiveness of Voting Methods: The Game between Margins and Weights / Vladimir Koltchinskii, Dmitriy Panchenko and Fernando Lozano -- Geometric Methods in the Analysis of Glivenko-Cantelli Classes / Shahar Mendelson -- Learning Relatively Small Classes / Shahar Mendelson -- On Agnostic Learning with {\{}0, *, 1{\}}-Valued and Real-Valued Hypotheses / Philip M. Long -- When Can Two Unsupervised Learners Achieve PAC Separation? / Paul W. Goldberg -- Strong Entropy Concentration, Game Theory and Algorithmic Randomness / Peter Grunwald -- Pattern Recognition and Density Estimation under the General i.i.d. Assumption / Ilia Nouretdinov, Volodya Vovk and Michael Vyugin / [and others] -- A General Dimension for Exact Learning / Jose L. Balcazar, Jorge Castro and David Guijarro -- Data-Dependent Margin-Based Generalization Bounds for Classification / Balazs Kegl, Tamas Linder and Gabor Lugosi -- Limitations of Learning Via Embeddings in Euclidean Half-Spaces / Shai Ben-David, Nadav Eiron and Hans Ulrich Simon -- Estimating the Optimal Margins of Embeddings in Euclidean Half Spaces / Jurgen Forster, Niels Schmitt and Hans Ulrich Simon -- A Generalized Representer Theorem / Bernhard Scholkopf, Ralf Herbrich and Alex J. Smola -- A Leave-One Out Cross Validation Bound for Kernel Methods with Applications in Learning / Tong Zhang -- Learning Additive Models Online with Fast Evaluating Kernels / Mark Herbster -- Geometric Bounds for Generalization in Boosting / Shie Mannor and Ron Meir -- Smooth Boosting and Learning with Malicious Noise / Rocco A. Servedio -- On Boosting with Optimal Poly-Bounded Distributions / Nader H. Bshouty and Dmitry Gavinsky -- Agnostic Boosting / Shai Ben-David, Philip M. Long and Yishay Mansour -- A Theoretical Analysis of Query Selection for Collaborative Filtering / Wee Sun Lee and Philip M. Long -- On Using Extended Statistical Queries to Avoid Membership Queries / Nader H. Bshouty and Vitaly Feldman -- Learning Monotone DNF from a Teacher That Almost Does Not Answer Membership Queries / Nader H. Bshouty and Nadav Eiron -- On Learning Montone DNF under Product Distributions / Rocco A. Servedio -- Learning Regular Sets with an Incomplete Membership Oracle / Nader Bshouty and Avi Owshanko -- Learning Rates for Q-Learning / Eyal Even-Dar and Yishay Mansour -- Optimizing Average Reward Using Discounted Rewards / Sham Kakade -- Bounds on Sample Size for Policy Evaluation in Markov Environments / Leonid Peshkin and Sayan Mukherjee.},
author = {Kakade, Sham},
booktitle = {Proceedings of the 14th Annual Conference on Computational Learning Theory and and 5th European Conference on Computational Learning Theory},
isbn = {3540423435},
pages = {605--615},
publisher = {Springer},
title = {{Optimizing Average Reward Using Discounted Rewards}},
url = {https://dl.acm.org/citation.cfm?id=755345},
year = {2001}
}
@book{Csiszar2011,
abstract = {2nd ed. Fully updated and revised edition of CsiszaÃÅr and KoÃàrner's classic book on information theory. Cover; Information Theory; Title; Copyright; Contents; Preface to the first edition; Preface to the second edition; Basic notation and conventions; Preliminaries on random variables and probability distributions; Introduction; Intuitive background; Informal description of the basic mathematical model; Measuring information; Multi-terminal systems; Part I Information measures in simple coding problems; 1 Source coding and hypothesis testing; information measures; Discussion; Problems; Postulational characterizations of entropy (Problems 1.11-1.14); Story of the results. 2 Types and typical sequencesDiscussion; Problems; Story of the results; 3 Formal properties of Shannon's information measures; Problems; Properties of informational divergence (Problems 3.17-3.20); Structural results on entropy (Problems 3.21-3.22); Story of the results; 4 Non-block source coding; Problems; General noiseless channels (Problems 4.20-4.22); Universal variable-length codes (Problems 4.23-4.26); Story of the results; 5 Blowing up lemma: a combinatorial digression; Problems; Story of the results; Part II Two-terminal systems; 6 The noisy channel coding problem; Discussion. ProblemsComparison of channels (Problems 6.16-6.18); Zero-error capacity and graphs (Problems 6.23-6.25); Story of the results; 7 Rate-distortion trade-off in source coding and the source-channel transmission problem; Discussion; Problems; Story of the results; 8 Computation of channel capacity and?-distortion rates; Problems; Story of the results; 9 A covering lemma and the error exponent in source coding; Problems; Graph entropy and convex corners; Story of the results; 10 A packing lemma and the error exponent in channel coding; Discussion; Problems; Compound DMCs (Problems 10.12-10.14). Reliability at R = 0 (Problems 10.20-10.23)Story of the results; 11 The compound channel revisited: zero-error information theory and extremal combinatorics; Discussion; Problems; Story of the results; 12 Arbitrarily varying channels; Discussion; Problems; Story of the results; Part III Multi-terminal systems; 13 Separate coding of correlated sources; Discussion; Problems; Story of the results; 14 Multiple-access channels; Discussion; Problems; Reduction of channel network problems (Problems 14.22-14.24); Story of the results; 15 Entropy and image size characterization; Discussion; Problems. Image size of arbitrary sets (Problems 15.4-15.5)More-than-three-component sources (Problems 15.16-15.21); Story of the results; 16 Source and channel networks; Discussion; Problems; Broadcast channels (Problems 16.8-16.12); Source networks with three inputs and one helper (Problems 16.13-16.18); Source networks with two helpers; General fidelity criteria (Problems 16.22-16.24); Common information (Problems 16.27-16.30); Miscellaneous source networks (Problems 16.31-16.33); Story of the results; 17 Information-theoretic security; 17.1 Basic concepts and tools.},
author = {CsiszaÃÅr, Imre and KoÃàrner, JaÃÅnos.},
isbn = {9780521196819},
pages = {499},
publisher = {Cambridge University Press},
title = {{Information theory : coding theorems for discrete memoryless systems}},
url = {https://www.cambridge.org/gb/academic/subjects/engineering/communications-and-signal-processing/information-theory-coding-theorems-discrete-memoryless-systems-2nd-edition?format=HB{\&}isbn=9780521196819},
year = {2011}
}
@article{Silver2017,
abstract = {Starting from zero knowledge and without human data, AlphaGo Zero was able to teach itself to play Go and to develop novel strategies that provide new insights into the oldest of games.},
author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature24270},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Silver et al. - 2017 - Mastering the game of Go without human knowledge(2).pdf:pdf},
issn = {0028-0836},
journal = {Nature},
keywords = {Computational science,Computer science,Reward},
month = {oct},
number = {7676},
pages = {354--359},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go without human knowledge}},
url = {http://www.nature.com/doifinder/10.1038/nature24270},
volume = {550},
year = {2017}
}
@techreport{Abbasi-Yadkori,
abstract = {We study average and total cost Markov decision problems with large state spaces. Since the computational and statistical cost of finding the optimal policy scales with the size of the state space, we focus on searching for near-optimality in a low-dimensional family of policies. In particular , we show that for problems with a Kullback-Leibler divergence cost function, we can recast policy optimization as a convex optimization and solve it approximately using a stochastic subgra-dient algorithm. This method scales in complexity with the family of policies but not the state space. We show that the performance of the resulting policy is close to the best in the low-dimensional family. We demonstrate the efficacy of our approach by optimizing a policy for budget allocation in crowd labeling, an important crowd-sourcing application.},
author = {Abbasi-Yadkori, Yasin and Bartlett, Peter L and Chen, Xi and Malek, Alan},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Abbasi-Yadkori et al. - Unknown - Large-Scale Markov Decision Problems with KL Control Cost and its Application to Crowdsourcing.pdf:pdf},
title = {{Large-Scale Markov Decision Problems with KL Control Cost and its Application to Crowdsourcing}}
}
@article{CongLuong,
abstract = {This paper presents a comprehensive literature review on applications of deep reinforcement learning in communications and networking. Modern networks, e.g., Internet of Things (IoT) and Unmanned Aerial Vehicle (UAV) networks, become more decentralized and autonomous. In such networks, network entities need to make decisions locally to maximize the network performance under uncertainty of network environment. Reinforcement learning has been efficiently used to enable the network entities to obtain the optimal policy including, e.g., decisions or actions, given their states when the state and action spaces are small. However, in complex and large-scale networks, the state and action spaces are usually large, and the reinforcement learning may not be able to find the optimal policy in reasonable time. Therefore, deep reinforcement learning, a combination of reinforcement learning with deep learning, has been developed to overcome the shortcomings. In this survey, we first give a tutorial of deep reinforcement learning from fundamental concepts to advanced models. Then, we review deep reinforcement learning approaches proposed to address emerging issues in communications and networking. The issues include dynamic network access, data rate control, wireless caching, data offloading, network security, and connectivity preservation which are all important to next generation networks such as 5G and beyond. Furthermore, we present applications of deep reinforcement learning for traffic routing, resource sharing, and data collection. Finally, we highlight important challenges, open issues, and future research directions of applying deep reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1810.07862v1},
author = {{Cong Luong}, Nguyen and {Thai Hoang}, Dinh and Gong, Shimin and Niyato, Dusit and Wang, Ping and Liang, Ying-Chang and {In Kim}, Dong},
eprint = {1810.07862v1},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cong Luong et al. - Unknown - Applications of Deep Reinforcement Learning in Communications and Networking A Survey.pdf:pdf},
keywords = {Deep reinforcement learning,caching,communications,data collection,data offloading,deep Q-learning,net-working,rate control,security,spectrum access},
title = {{Applications of Deep Reinforcement Learning in Communications and Networking: A Survey}},
url = {https://arxiv.org/pdf/1810.07862.pdf},
year = {2018}
}
@misc{OpenAI2019,
author = {OpenAI},
doi = {https://blog.openai.com/openai-five/},
title = {{OpenAI Five}},
url = {https://openai.com/five/},
urldate = {2019-05-29},
year = {2019}
}
@article{Lippman1975,
abstract = {We consider the problem of controlling M/M/c queuing systems. By providing a new definition of the time of transition, we enlarge the standard set of decision epochs and obtain a preferred version ...},
author = {Lippman, Steven A.},
doi = {10.1287/opre.23.4.687},
issn = {0030-364X},
journal = {Operations Research},
month = {aug},
number = {4},
pages = {687--710},
publisher = { INFORMS },
title = {{Applying a New Device in the Optimization of Exponential Queuing Systems}},
url = {http://pubsonline.informs.org/doi/abs/10.1287/opre.23.4.687},
volume = {23},
year = {1975}
}
@inproceedings{Ramirez-Hernandez2007b,
author = {Ramirez-Hernandez, Jose A. and Fernandez, Emmanuel},
booktitle = {2007 IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning},
doi = {10.1109/ADPRL.2007.368189},
isbn = {1-4244-0706-0},
month = {apr},
pages = {201--208},
publisher = {IEEE},
title = {{An Approximate Dynamic Programming Approach for Job Releasing and Sequencing in a Reentrant Manufacturing Line}},
url = {https://ieeexplore.ieee.org/document/4220834/},
year = {2007}
}
@inproceedings{Schulman2015,
abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
archivePrefix = {arXiv},
arxivId = {1502.05477},
author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
booktitle = {Proceeding ICML'15},
eprint = {1502.05477},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schulman et al. - 2015 - Trust Region Policy Optimization.pdf:pdf},
month = {feb},
pages = {1889--1897},
title = {{Trust Region Policy Optimization}},
url = {http://arxiv.org/abs/1502.05477},
year = {2015}
}
@inproceedings{Ramirez-Hernandez,
author = {Ramirez-Hernandez, J.A. and Fernandez, E.},
booktitle = {Proceedings of the 44th IEEE Conference on Decision and Control},
doi = {10.1109/CDC.2005.1582481},
isbn = {0-7803-9567-0},
pages = {2158--2163},
publisher = {IEEE},
title = {{A Case Study in Scheduling Reentrant Manufacturing Lines: Optimal and Simulation-Based Approaches}},
url = {http://ieeexplore.ieee.org/document/1582481/},
year = {2005}
}
@article{Meketon1982,
author = {Meketon, Marc S and Heidelberger, Philip},
journal = {Management Science},
number = {2},
pages = {173--182},
title = {{A Renewal Theoretic Approach to Bias Reduction in Regenerative Simulations}},
url = {https://search-proquest-com.proxy.library.cornell.edu/docview/213220992?pq-origsite=summon},
volume = {28},
year = {1982}
}
@article{Taylor1993,
author = {Taylor, L. M. and Williams, R. J.},
doi = {10.1007/BF01292674},
issn = {0178-8051},
journal = {Probability Theory and Related Fields},
month = {sep},
number = {3},
pages = {283--317},
publisher = {Springer-Verlag},
title = {{Existence and uniqueness of semimartingale reflecting Brownian motions in an orthant}},
url = {http://link.springer.com/10.1007/BF01292674},
volume = {96},
year = {1993}
}
@article{Bertsimas1994,
author = {Bertsimas, Dimitris and Paschalidis, Ioannis Ch. and Tsitsiklis, John N},
journal = {The Annals of Applied Probability},
number = {1},
pages = {43--75},
title = {{Optimization of Multiclass Queueing Networks: Polyhedral and Nonlinear Characterizations of Achievable Performance}},
url = {http://ieeexplore.ieee.org/document/1094075/},
volume = {4},
year = {1994}
}
@article{Harrison1990,
address = {New York, NY},
author = {Harrison, J. Michael and Wein, Lawrence M.},
doi = {10.1007/978-1-4684-0302-2},
isbn = {978-1-4684-0304-6},
journal = {Operations Research},
number = {6},
pages = {1052--1064},
publisher = {Springer US},
series = {Graduate Texts in Mathematics},
title = {{Scheduling Networks of Queues: Heavy Traffic Analysis of a Two-Station Closed Network}},
url = {http://link.springer.com/10.1007/978-1-4684-0302-2},
volume = {38},
year = {1990}
}
@article{Brochu2010,
abstract = {We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments---active user modelling with preferences, and hierarchical reinforcement learning---and a discussion of the pros and cons of Bayesian optimization based on our experiences.},
archivePrefix = {arXiv},
arxivId = {1012.2599},
author = {Brochu, Eric and Cora, Vlad M. and de Freitas, Nando},
eprint = {1012.2599},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brochu, Cora, de Freitas - 2010 - A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Model.pdf:pdf},
month = {dec},
title = {{A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning}},
url = {http://arxiv.org/abs/1012.2599},
year = {2010}
}
@article{Bertsimas2015,
author = {Bertsimas, Dimitris and Nasrabadi, Ebrahim and Paschalidis, Ioannis Ch.},
doi = {10.1109/TAC.2014.2352711},
issn = {0018-9286},
journal = {IEEE Transactions on Automatic Control},
month = {mar},
number = {3},
pages = {715--728},
title = {{Robust Fluid Processing Networks}},
url = {http://ieeexplore.ieee.org/document/6887315/},
volume = {60},
year = {2015}
}
@article{Gottesman2019,
abstract = {In this Comment, we provide guidelines for reinforcement learning for decisions about patient treatment that we hope will accelerate the rate at which observational cohorts can inform healthcare practice in a safe, risk-conscious manner.},
author = {Gottesman, Omer and Johansson, Fredrik and Komorowski, Matthieu and Faisal, Aldo and Sontag, David and Doshi-Velez, Finale and Celi, Leo Anthony},
doi = {10.1038/s41591-018-0310-5},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gottesman et al. - 2019 - Guidelines for reinforcement learning in healthcare.pdf:pdf},
issn = {1078-8956},
journal = {Nature Medicine},
keywords = {Health care,Machine learning},
month = {jan},
number = {1},
pages = {16--18},
publisher = {Nature Publishing Group},
title = {{Guidelines for reinforcement learning in healthcare}},
url = {http://www.nature.com/articles/s41591-018-0310-5},
volume = {25},
year = {2019}
}
@article{Laws1990,
abstract = {{\textless}p{\textgreater}This paper is concerned with the problem of optimally scheduling a multiclass open queueing network with four single-server stations in which dynamic control policies are permitted. Under the assumption that the system is heavily loaded, the original scheduling problem can be approximated by a dynamic control problem involving Brownian motion. We reformulate and solve this problem and, from the interpretation of the solution, we obtain two dynamic scheduling policies for our queueing network. We compare the performance of these policies with two static scheduling policies and a lower bound via simulation. Our results suggest that under either dynamic policy the system, at least when heavily loaded, exhibits the form of resource pooling given by the solution to the approximating control problem. Furthermore, even when lightly loaded the system performs better under the dynamic policies than under either static policy.{\textless}/p{\textgreater}},
author = {Laws, C. N. and Louth, G. M.},
doi = {10.1017/S0269964800001492},
issn = {0269-9648},
journal = {Probability in the Engineering and Informational Sciences},
month = {jan},
number = {1},
pages = {131--156},
publisher = {Cambridge University Press},
title = {{Dynamic Scheduling of a Four-Station Queueing Network}},
url = {https://www.cambridge.org/core/product/identifier/S0269964800001492/type/journal{\_}article},
volume = {4},
year = {1990}
}
@article{Resnick2018,
abstract = {Model-free reinforcement learning (RL) requires a large number of trials to learn a good policy, especially in environments with sparse rewards. We explore a method to improve the sample efficiency when we have access to demonstrations. Our approach, Backplay, uses a single demonstration to construct a curriculum for a given task. Rather than starting each training episode in the environment's fixed initial state, we start the agent near the end of the demonstration and move the starting point backwards during the course of training until we reach the initial state. Our contributions are that we analytically characterize the types of environments where Backplay can improve training speed, demonstrate the effectiveness of Backplay both in large grid worlds and a complex four player zero-sum game (Pommerman), and show that Backplay compares favorably to other competitive methods known to improve sample efficiency. This includes reward shaping, behavioral cloning, and reverse curriculum generation.},
archivePrefix = {arXiv},
arxivId = {1807.06919},
author = {Resnick, Cinjon and Raileanu, Roberta and Kapoor, Sanyam and Peysakhovich, Alexander and Cho, Kyunghyun and Bruna, Joan},
eprint = {1807.06919},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Resnick et al. - 2018 - Backplay {\&}quotMan muss immer umkehren{\&}quot.pdf:pdf},
month = {jul},
title = {{Backplay: "Man muss immer umkehren"}},
url = {http://arxiv.org/abs/1807.06919},
year = {2018}
}
@article{Silver2016,
abstract = {A computer Go program based on deep neural networks defeats a human professional player to achieve one of the grand challenges of artificial intelligence.},
author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature16961},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Silver et al. - 2016 - Mastering the game of Go with deep neural networks and tree search.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
keywords = {Computational science,Computer science,Reward},
month = {jan},
number = {7587},
pages = {484--489},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go with deep neural networks and tree search}},
url = {http://www.nature.com/articles/nature16961},
volume = {529},
year = {2016}
}
@article{Meyn1997,
author = {Meyn, Sean},
journal = {Lectures in applied mathematics-American Mathematical Society},
pages = {175--200},
title = {{Stability and Optimization of Queueing Networks and Their Fluid Models}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.38.8399},
volume = {33},
year = {1997}
}
@article{Moritz2018,
abstract = {The next generation of AI applications will continuously interact with the environment and learn from these interactions. These applications impose new and demanding systems requirements, both in terms of performance and flexibility. In this paper, we consider these requirements and present Ray---a distributed system to address them. Ray implements a unified interface that can express both task-parallel and actor-based computations, supported by a single dynamic execution engine. To meet the performance requirements, Ray employs a distributed scheduler and a distributed and fault-tolerant store to manage the system's control state. In our experiments, we demonstrate scaling beyond 1.8 million tasks per second and better performance than existing specialized systems for several challenging reinforcement learning applications.},
archivePrefix = {arXiv},
arxivId = {1712.05889},
author = {Moritz, Philipp and Nishihara, Robert and Wang, Stephanie and Tumanov, Alexey and Liaw, Richard and Liang, Eric and Elibol, Melih and Yang, Zongheng and Paul, William and Jordan, Michael I. and Stoica, Ion},
eprint = {1712.05889},
month = {dec},
title = {{Ray: A Distributed Framework for Emerging AI Applications}},
url = {http://arxiv.org/abs/1712.05889},
year = {2018}
}
@misc{Coady2017,
author = {Coady, Patrick},
title = {{AI Gym Workout}},
url = {https://learningai.io/projects/2017/07/28/ai-gym-workout.html},
urldate = {2019-02-23},
year = {2017}
}
@article{Veatch2015,
abstract = {This paper uses approximate linear programming (ALP) to compute average cost bounds for queueing network control problems. Like most approximate dynamic programming (ADP) methods, ALP approximates the differential cost by a linear form. New types of approximating functions are identified that offer more accuracy than previous ALP studies or other performance bound methods. The structure of the infinite constraint set is exploited to reduce it to a more manageable set. When needed, constraint sampling and truncation methods are also developed. Numerical experiments show that the LPs using quadratic approximating functions can be easily solved on examples with up to 17 buffers. Using additional functions reduced the error to 1‚Ä?{\%} at the cost of larger LPs. These ALPs were solved for systems with up to 6‚Ä?1 buffers, depending on the functions used. The method computes bounds much faster than value iteration. It also gives some insights into policies. The ALPs do not scale to very large problems, but they offer more accurate bounds than other methods and the simplicity of just solving an LP.},
author = {Veatch, Michael H.},
doi = {10.1016/j.cor.2015.04.014},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Veatch - 2015 - Approximate linear programming for networks Average cost bounds.pdf:pdf},
issn = {03050548},
journal = {Computers {\&} Operations Research},
month = {nov},
pages = {32--45},
title = {{Approximate linear programming for networks: Average cost bounds}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0305054815000945 http://linkinghub.elsevier.com/retrieve/pii/S0305054815000945},
volume = {63},
year = {2015}
}
@article{Mnih2015,
abstract = {An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - 2015 - Human-level control through deep reinforcement learning.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
keywords = {Computer science},
month = {feb},
number = {7540},
pages = {529--533},
publisher = {Nature Publishing Group},
title = {{Human-level control through deep reinforcement learning}},
url = {http://www.nature.com/articles/nature14236},
volume = {518},
year = {2015}
}
@article{Rudolf2011,
abstract = {We prove explicit, i.e. non-asymptotic, error bounds for Markov chain Monte Carlo methods. The problem is to compute the expectation of a function f with respect to a measure {\{}$\backslash$pi{\}}. Different convergence properties of Markov chains imply different error bounds. For uniformly ergodic and reversible Markov chains we prove a lower and an upper error bound with respect to the L2 -norm of f . If there exists an L2 -spectral gap, which is a weaker convergence property than uniform ergodicity, then we show an upper error bound with respect to the Lp -norm of f for p {\textgreater} 2. Usually a burn-in period is an efficient way to tune the algorithm. We provide and justify a recipe how to choose the burn-in period. The error bounds are applied to the problem of the integration with respect to a possibly unnormalized density. More precise, we consider the integration with respect to log-concave densities and the integration over convex bodies. By the use of the Metropolis algorithm based on a ball walk and the hit-and-run algorithm it is shown that both problems are polynomial tractable.},
archivePrefix = {arXiv},
arxivId = {1108.3201},
author = {Rudolf, Daniel},
eprint = {1108.3201},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rudolf - 2011 - Explicit error bounds for Markov chain Monte Carlo.pdf:pdf},
month = {aug},
title = {{Explicit error bounds for Markov chain Monte Carlo}},
url = {http://arxiv.org/abs/1108.3201},
year = {2011}
}
@book{Sennott1999,
abstract = {A path-breaking account of Markov decision processes-theory and computation. This book's clear presentation of theory, numerous chapter-end problems, and development of a unified method for the computation of optimal policies in both discrete and continuous time make it an excellent course text for graduate students and advanced undergraduates. Its comprehensive coverage of important recent advances in stochastic dynamic programming makes it a valuable working resource for operations research professionals, management scientists, engineers, and others. Stochastic Dynamic Programming and the Co. Optimization criteria -- Finite horizon optimization -- Infinite horizon discounted cost optimization -- An inventory model -- Average cost optimization for finite state spaces -- Average cost optimization theory for countable state spaces -- Computation of average cost optimal policies for infinite state spaces -- Optimization under actions at selected epochs -- Average cost optimization of continuous time processes -- Appendices -- Bibliography -- Index.},
author = {Sennott, Linn I.},
isbn = {9780470317037},
pages = {328},
publisher = {Wiley},
title = {{Stochastic dynamic programming and the control of queueing systems}},
year = {1999}
}
@techreport{J2016,
abstract = {A recent goal in the Reinforcement Learning (RL) framework is to choose a sequence of actions or a policy to maximize the reward collected or minimize the regret incurred in a finite time horizon. For several RL problems in operation research and optimal control, the optimal policy of the underlying Markov Decision Process (MDP) is characterized by a known structure. The current state of the art algorithms do not utilize this known structure of the optimal policy while minimizing regret. In this work, we develop new RL algorithms that exploit the structure of the optimal policy to minimize regret. Numerical experiments on MDPs with structured optimal policies show that our algorithms have better performance, are easy to implement, have a smaller run-time and require less number of random number generations.},
archivePrefix = {arXiv},
arxivId = {1608.04929v1},
author = {J, Prabuchandran K and Bodas, Tejas and Tulabandhula, Theja},
eprint = {1608.04929v1},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/J, Bodas, Tulabandhula - 2016 - Reinforcement Learning algorithms for regret minimization in structured Markov Decision Processes.pdf:pdf},
title = {{Reinforcement Learning algorithms for regret minimization in structured Markov Decision Processes}},
url = {https://arxiv.org/pdf/1608.04929.pdf},
year = {2016}
}
@inproceedings{Mao2016,
address = {New York, New York, USA},
author = {Mao, Hongzi and Alizadeh, Mohammad and Menache, Ishai and Kandula, Srikanth},
booktitle = {Proceedings of the 15th ACM Workshop on Hot Topics in Networks  - HotNets '16},
doi = {10.1145/3005745.3005750},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mao et al. - 2016 - Resource Management with Deep Reinforcement Learning.pdf:pdf},
isbn = {9781450346610},
pages = {50--56},
publisher = {ACM Press},
title = {{Resource Management with Deep Reinforcement Learning}},
url = {http://dl.acm.org/citation.cfm?doid=3005745.3005750},
year = {2016}
}
@article{Harrison1989,
author = {Harrison, J. Michael and Wein, Lawrence M.},
doi = {10.1007/BF01225319},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Harrison, Wein - 1989 - Scheduling networks of queues Heavy traffic analysis of a simple open network.pdf:pdf},
issn = {0257-0130},
journal = {Queueing Systems},
month = {dec},
number = {4},
pages = {265--279},
publisher = {Kluwer Academic Publishers},
title = {{Scheduling networks of queues: Heavy traffic analysis of a simple open network}},
url = {http://link.springer.com/10.1007/BF01225319},
volume = {5},
year = {1989}
}
@article{Bellemare2013,
abstract = {In this article we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by developing and benchmarking domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. All of the software, including the benchmark agents, is publicly available.},
author = {Bellemare, Marc G. and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
journal = {Journal of Artificial Intelligence Research},
number = {1},
pages = {253--279},
publisher = {AI Access Foundation},
title = {{The arcade learning environment: an evaluation platform for general agents}},
url = {https://dl.acm.org/citation.cfm?id=2566979},
volume = {47},
year = {2013}
}
@article{Wein1990,
author = {Wein, Lawrence M.},
doi = {10.1287/moor.15.2.215},
issn = {0364-765X},
journal = {Mathematics of Operations Research},
keywords = {Brownian approximations,heavy traffic analysis,networks of queues,singular control},
month = {may},
number = {2},
pages = {215--242},
publisher = {INFORMS},
title = {{Optimal Control of a Two-Station Brownian Network}},
url = {http://pubsonline.informs.org/doi/abs/10.1287/moor.15.2.215},
volume = {15},
year = {1990}
}
@article{Terekhov2014,
abstract = {Within the combinatorial scheduling community, there has been an increasing interest in modelling and solving scheduling problems in dynamic environments. Such problems have also been considered in the field of queueing theory, but very few papers take advantage of developments in both areas, and literature surveys on dynamic scheduling usually make no mention of queueing approaches. In this paper, we provide an overview of queueing-theoretic models and methods that are relevant to scheduling in dynamic settings. This paper provides a context for investigating the integration of queueing theory and scheduling approaches with the goal of more effectively solving scheduling problems arising in dynamic environments.},
author = {Terekhov, Daria and Down, Douglas G. and Beck, J. Christopher},
doi = {10.1016/J.SORMS.2014.09.001},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Terekhov, Down, Beck - 2014 - Queueing-theoretic approaches for dynamic scheduling A survey.pdf:pdf},
issn = {1876-7354},
journal = {Surveys in Operations Research and Management Science},
month = {jul},
number = {2},
pages = {105--129},
publisher = {Elsevier},
title = {{Queueing-theoretic approaches for dynamic scheduling: A survey}},
url = {https://www.sciencedirect.com/science/article/pii/S1876735414000233},
volume = {19},
year = {2014}
}
@article{Bertsimas2017,
abstract = {Dynamic resource allocation (DRA) problems constitute an important class of dynamic stochastic optimization problems that arise in many real-world applications. DRA problems are notoriously difficult to solve since they combine stochastic dynamics with intractably large state and action spaces. Although the artificial intelligence and operations research communities have independently proposed two successful frameworks for solving such problems‚ÄîMonte Carlo tree search (MCTS) and rolling horizon optimization (RHO), respectively‚Äîthe relative merits of these two approaches are not well understood. In this paper, we adapt MCTS and RHO to two problems ‚Ä?a problem inspired by tactical wildfire management and a classical problem involving the control of queueing networks ‚Ä?and undertake an extensive computational study comparing the two methods on large scale instances of both problems in terms of both the state and the action spaces. Both methods are able to greatly improve on a baseline, problem-specific heuristic. On smaller instances, the MCTS and RHO approaches perform comparably, but RHO outperforms MCTS as the size of the problem increases for a fixed computational budget.},
author = {Bertsimas, Dimitris and Griffith, J. Daniel and Gupta, Vishal and Kochenderfer, Mykel J. and Mi{\v{s}}i{\'{c}}, Velibor V.},
doi = {10.1016/J.EJOR.2017.05.032},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bertsimas et al. - 2017 - A comparison of Monte Carlo tree search and rolling horizon optimization for large-scale dynamic resource allo.pdf:pdf},
issn = {0377-2217},
journal = {European Journal of Operational Research},
month = {dec},
number = {2},
pages = {664--678},
publisher = {North-Holland},
title = {{A comparison of Monte Carlo tree search and rolling horizon optimization for large-scale dynamic resource allocation problems}},
url = {https://www.sciencedirect.com/science/article/pii/S0377221717304605},
volume = {263},
year = {2017}
}
@inproceedings{Ramirez-Hernandez2010,
author = {Ramirez-Hernandez, Jose A. and Fernandez, Emmanuel},
booktitle = {49th IEEE Conference on Decision and Control (CDC)},
doi = {10.1109/CDC.2010.5717523},
isbn = {978-1-4244-7745-6},
month = {dec},
pages = {3944--3949},
publisher = {IEEE},
title = {{Optimization of Preventive Maintenance scheduling in semiconductor manufacturing models using a simulation-based Approximate Dynamic Programming approach}},
url = {http://ieeexplore.ieee.org/document/5717523/},
year = {2010}
}
@book{Sutton2018,
abstract = {Second edition. "Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."-- Machine generated contents note: 1.Introduction -- 1.1.Reinforcement Learning -- 1.2.Examples -- 1.3.Elements of Reinforcement Learning -- 1.4.Limitations and Scope -- 1.5.An Extended Example: Tic-Tac-Toe -- 1.6.Summary -- 1.7.Early History of Reinforcement Learning -- 2.Multi-armed Bandits -- 2.1.A k-armed Bandit Problem -- 2.2.Action-value Methods -- 2.3.The 10-armed Testbed -- 2.4.Incremental Implementation -- 2.5.Tracking a Nonstationary Problem -- 2.6.Optimistic Initial Values -- 2.7.Upper-Confidence-Bound Action Selection -- 2.8.Gradient Bandit Algorithms -- 2.9.Associative Search (Contextual Bandits) -- 2.10.Summary -- 3.Finite Markov Decision Processes -- 3.1.The Agent-Environment Interface -- 3.2.Goals and Rewards -- 3.3.Returns and Episodes -- 3.4.Unified Notation for Episodic and Continuing Tasks -- 3.5.Policies and Value Functions -- 3.6.Optimal Policies and Optimal Value Functions -- 3.7.Optimality and Approximation -- 3.8.Summary -- 4.Dynamic Programming Note continued: 4.1.Policy Evaluation (Prediction) -- 4.2.Policy Improvement -- 4.3.Policy Iteration -- 4.4.Value Iteration -- 4.5.Asynchronous Dynamic Programming -- 4.6.Generalized Policy Iteration -- 4.7.Efficiency of Dynamic Programming -- 4.8.Summary -- 5.Monte Carlo Methods -- 5.1.Monte Carlo Prediction -- 5.2.Monte Carlo Estimation of Action Values -- 5.3.Monte Carlo Control -- 5.4.Monte Carlo Control without Exploring Starts -- 5.5.Off-policy Prediction via Importance Sampling -- 5.6.Incremental Implementation -- 5.7.Off-policy Monte Carlo Control -- 5.8.*Discounting-aware Importance Sampling -- 5.9.*Per-decision Importance Sampling -- 5.10.Summary -- 6.Temporal-Difference Learning -- 6.1.TD Prediction -- 6.2.Advantages of TD Prediction Methods -- 6.3.Optimality of TD(0) -- 6.4.Sarsa: On-policy TD Control -- 6.5.Q-learning: Off-policy TD Control -- 6.6.Expected Sarsa -- 6.7.Maximization Bias and Double Learning Note continued: 6.8.Games, Afterstates, and Other Special Cases -- 6.9.Summary -- 7.n-step Bootstrapping -- 7.1.n-step TD Prediction -- 7.2.n-step Sarsa -- 7.3.n-step Off-policy Learning -- 7.4.*Per-decision Methods with Control Variates -- 7.5.Off-policy Learning Without Importance Sampling: The n-step Tree Backup Algorithm -- 7.6.*A Unifying Algorithm: n-step Q(u) -- 7.7.Summary -- 8.Planning and Learning with Tabular Methods -- 8.1.Models and Planning -- 8.2.Dyna: Integrated Planning, Acting, and Learning -- 8.3.When the Model Is Wrong -- 8.4.Prioritized Sweeping -- 8.5.Expected vs. Sample Updates -- 8.6.Trajectory Sampling -- 8.7.Real-time Dynamic Programming -- 8.8.Planning at Decision Time -- 8.9.Heuristic Search -- 8.10.Rollout Algorithms -- 8.11.Monte Carlo Tree Search -- 8.12.Summary of the Chapter -- 8.13.Summary of Part I: Dimensions -- 9.On-policy Prediction with Approximation -- 9.1.Value-function Approximation -- 9.2.The Prediction Objective (VE) Note continued: 9.3.Stochastic-gradient and Semi-gradient Methods -- 9.4.Linear Methods -- 9.5.Feature Construction for Linear Methods -- 9.5.1.Polynomials -- 9.5.2.Fourier Basis -- 9.5.3.Coarse Coding -- 9.5.4.Tile Coding -- 9.5.5.Radial Basis Functions -- 9.6.Selecting Step-Size Parameters Manually -- 9.7.Nonlinear Function Approximation: Artificial Neural Networks -- 9.8.Least-Squares TD -- 9.9.Memory-based Function Approximation -- 9.10.Kernel-based Function Approximation -- 9.11.Looking Deeper at On-policy Learning: Interest and Emphasis -- 9.12.Summary -- 10.On-policy Control with Approximation -- 10.1.Episodic Semi-gradient Control -- 10.2.Semi-gradient n-step Sarsa -- 10.3.Average Reward: A New Problem Setting for Continuing Tasks -- 10.4.Deprecating the Discounted Setting -- 10.5.Differential Semi-gradient n-step Sarsa -- 10.6.Summary -- 11.*Off-policy Methods with Approximation -- 11.1.Semi-gradient Methods -- 11.2.Examples of Off-policy Divergence Note continued: 11.3.The Deadly Triad -- 11.4.Linear Value-function Geometry -- 11.5.Gradient Descent in the Bellman Error -- 11.6.The Bellman Error is Not Learnable -- 11.7.Gradient-TD Methods -- 11.8.Emphatic-TD Methods -- 11.9.Reducing Variance -- 11.10.Summary -- 12.Eligibility Traces -- 12.1.The A-return -- 12.2.TD(A) -- 12.3.n-step Truncated A-return Methods -- 12.4.Redoing Updates: Online A-return Algorithm -- 12.5.True Online TD(A) -- 12.6.*Dutch Traces in Monte Carlo Learning -- 12.7.Sarsa(A) -- 12.8.Variable A and ry -- 12.9.Off-policy Traces with Control Variates -- 12.10.Watkins's Q(A) to Tree-Backup(A) -- 12.11.Stable Off-policy Methods with Traces -- 12.12.Implementation Issues -- 12.13.Conclusions -- 13.Policy Gradient Methods -- 13.1.Policy Approximation and its Advantages -- 13.2.The Policy Gradient Theorem -- 13.3.REINFORCE: Monte Carlo Policy Gradient -- 13.4.REINFORCE with Baseline -- 13.5.Actor-Critic Methods Note continued: 13.6.Policy Gradient for Continuing Problems -- 13.7.Policy Parameterization for Continuous Actions -- 13.8.Summary -- 14.Psychology -- 14.1.Prediction and Control -- 14.2.Classical Conditioning -- 14.2.1.Blocking and Higher-order Conditioning -- 14.2.2.The Rescorla-Wagner Model -- 14.2.3.The TD Model -- 14.2.4.TD Model Simulations -- 14.3.Instrumental Conditioning -- 14.4.Delayed Reinforcement -- 14.5.Cognitive Maps -- 14.6.Habitual and Goal-directed Behavior -- 14.7.Summary -- 15.Neuroscience -- 15.1.Neuroscience Basics -- 15.2.Reward Signals, Reinforcement Signals, Values, and Prediction Errors -- 15.3.The Reward Prediction Error Hypothesis -- 15.4.Dopamine -- 15.5.Experimental Support for the Reward Prediction Error Hypothesis -- 15.6.TD Error/Dopamine Correspondence -- 15.7.Neural Actor-Critic -- 15.8.Actor and Critic Learning Rules -- 15.9.Hedonistic Neurons -- 15.10.Collective Reinforcement Learning -- 15.11.Model-based Methods in the Brain Note continued: 15.12.Addiction -- 15.13.Summary -- 16.Applications and Case Studies -- 16.1.TD-Gammon -- 16.2.Samuel's Checkers Player -- 16.3.Watson's Daily-Double Wagering -- 16.4.Optimizing Memory Control -- 16.5.Human-level Video Game Play -- 16.6.Mastering the Game of Go -- 16.6.1.AlphaGo -- 16.6.2.AlphaGo Zero -- 16.7.Personalized Web Services -- 16.8.Thermal Soaring -- 17.Frontiers -- 17.1.General Value Functions and Auxiliary Tasks -- 17.2.Temporal Abstraction via Options -- 17.3.Observations and State -- 17.4.Designing Reward Signals -- 17.5.Remaining Issues -- 17.6.Experimental Support for the Reward Prediction Error Hypothesis.},
author = {Sutton, Richard S. and Barto, Andrew G.},
edition = {2nd},
isbn = {9780262039246},
pages = {526},
publisher = {MIT press},
title = {{Reinforcement learning : an introduction}},
url = {https://mitpress.mit.edu/books/reinforcement-learning-second-edition},
year = {2018}
}
@article{Baxter2001,
abstract = {Gradient-based approaches to direct policy search in reinforcement learning have received much recent attention as a means to solve problems of partial observability and to avoid some of the problems associated with policy degradation in value-function methods. In this paper we introduce {\`{E}}{\c{C}}{\AA}{\AA}{\`{E}}, a simulation-based algorithm for generating a biased estimate of the gradient of the average reward in Partially Observable Markov Decision Processes ({\`{E}}{\c{C}}{\AA}{\AA}{\`{E}}s) controlled by parameterized stochastic policies. A similar algorithm was proposed by Kimura, Yamamura, and Kobayashi (1995). The algorithm's chief advantages are that it requires storage of only twice the number of policy parameters, uses one free parameter ¬¨ ¬æ ¬º ¬Ω¬µ (which has a natural interpretation in terms of bias-variance trade-off), and requires no knowledge of the underlying state. We prove convergence of {\`{E}}{\c{C}}{\AA}{\AA}{\`{E}}, and show how the correct choice of the parameter ¬¨ is related to the mixing time of the controlled {\`{E}}{\c{C}}{\AA}{\AA}{\`{E}}. We briefly describe extensions of {\`{E}}{\c{C}}{\AA}{\AA}{\`{E}} to controlled Markov chains, continuous state, observation and control spaces, multiple-agents, higher-order derivatives, and a version for training stochastic policies with internal states. In a companion paper (Baxter, Bartlett, {\&} Weaver, 2001) we show how the gradient estimates generated by {\`{E}}{\c{C}}{\AA}{\AA}{\`{E}} can be used in both a traditional stochastic gradient algorithm and a conjugate-gradient procedure to find local optima of the average reward.},
author = {Baxter, Jonathan and Bartlett, Peter L},
journal = {Journal of Artificial Intelligence Research},
pages = {319--350},
title = {{Infinite-Horizon Policy-Gradient Estimation}},
volume = {15},
year = {2001}
}
@techreport{Greensmith2004,
abstract = {Policy gradient methods for reinforcement learning avoid some of the undesirable properties of the value function approaches, such as policy degradation (Baxter and Bartlett, 2001). However, the variance of the performance gradient estimates obtained from the simulation is sometimes excessive. In this paper, we consider variance reduction methods that were developed for Monte Carlo estimates of integrals. We study two commonly used policy gradient techniques, the baseline and actor-critic methods, from this perspective. Both can be interpreted as additive control variate variance reduction methods. We consider the expected average reward performance measure, and we focus on the GPOMDP algorithm for estimating performance gradients in partially observable Markov decision processes controlled by stochastic reactive policies. We give bounds for the estimation error of the gradient estimates for both baseline and actor-critic algorithms, in terms of the sample size and mixing properties of the controlled system. For the baseline technique, we compute the optimal baseline, and show that the popular approach of using the average reward to define the baseline can be suboptimal. For actor-critic algorithms, we show that using the true value function as the critic can be suboptimal. We also discuss algorithms for estimating the optimal baseline and approximate value function.},
author = {Greensmith, Evan and Bartlett, Peter L and Edu, Bartlett@stat Berkeley and Baxter, Jonathan},
booktitle = {Journal of Machine Learning Research},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Greensmith et al. - 2004 - Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning.pdf:pdf},
keywords = {GPOMDP,actor-critic,baseline,policy gradient,reinforcement learning},
pages = {1471--1530},
title = {{Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning}},
url = {http://jmlr.csail.mit.edu/papers/volume5/greensmith04a/greensmith04a.pdf},
volume = {5},
year = {2004}
}
@book{Meyn2009,
address = {Cambridge},
author = {Meyn, Sean and Tweedie, Richard L.},
doi = {10.1017/CBO9780511626630},
edition = {2nd},
isbn = {9780511626630},
pages = {531},
publisher = {Cambridge University Press},
title = {{Markov Chains and Stochastic Stability}},
url = {http://ebooks.cambridge.org/ref/id/CBO9780511626630},
year = {2009}
}
@inproceedings{Abbasi_Yadkori2014a,
author = {Abbasi-Yadkori, Yasin and Bartlett, Peter and Malek, Alan},
booktitle = {Proceeding ICML'14 Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
pages = {496--504},
title = {{Linear programming for large-scale Markov decision problems}},
url = {https://dl.acm.org/citation.cfm?id=3044948},
year = {2014}
}
@inproceedings{Todorov2012,
author = {Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
booktitle = {2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2012.6386109},
isbn = {978-1-4673-1736-8},
month = {oct},
pages = {5026--5033},
publisher = {IEEE},
title = {{MuJoCo: A physics engine for model-based control}},
url = {http://ieeexplore.ieee.org/document/6386109/},
year = {2012}
}
@book{Bertsekas2013,
address = {Belmont, MA},
archivePrefix = {arXiv},
arxivId = {1608.01670},
author = {Bertsekas, Dimitri},
eprint = {1608.01670},
publisher = {Athena Scientific},
title = {{Abstract Dynamic Programming}},
url = {http://arxiv.org/abs/1608.01670},
year = {2013}
}
@article{Marbach2003a,
abstract = {This paper proposes a simulation-based algorithm for optimizing the$\backslash$naverage reward in a finite-state Markov reward process that depends$\backslash$non a set of parameters. As a special case, the method applies to$\backslash$nMarkov decision processes where optimization takes place within a$\backslash$nparametrized set of policies. The algorithm relies on the regenerative$\backslash$nstructure of finite-state Markov processes, involves the simulation$\backslash$nof a single sample path, and can be implemented online. A convergence$\backslash$nresult (with probability 1) is provided.},
author = {Marbach, Peter and Tsitsiklis, John N.},
doi = {10.1023/A:1022145020786},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Marbach, Tsitsiklis - 2003 - Approximate gradient methods in policy-space optimization of Markov reward processes.pdf:pdf},
issn = {09246703},
journal = {Discrete Event Dynamic Systems: Theory and Applications},
keywords = {Markov reward processes,Policy-space optimization,Simulation-based optimization},
month = {jan},
number = {1-2},
pages = {111--148},
title = {{Approximate gradient methods in policy-space optimization of Markov reward processes}},
volume = {13},
year = {2003}
}
@article{Ouelhadj2009,
author = {Ouelhadj, Djamila and Petrovic, Sanja},
doi = {10.1007/s10951-008-0090-8},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ouelhadj, Petrovic - 2009 - A survey of dynamic scheduling in manufacturing systems.pdf:pdf},
issn = {1094-6136},
journal = {Journal of Scheduling},
month = {aug},
number = {4},
pages = {417--431},
publisher = {Springer US},
title = {{A survey of dynamic scheduling in manufacturing systems}},
url = {http://link.springer.com/10.1007/s10951-008-0090-8},
volume = {12},
year = {2009}
}
@techreport{Braylan2015,
abstract = {We show that setting a reasonable frame skip can be critical to the performance of agents learning to play Atari 2600 games. In all of the six games in our experiments, frame skip is a strong determinant of success. For two of these games, setting a large frame skip leads to state-of-the-art performance. The rate at which an agent interacts with its environment may be critical to its success. In the Arcade Learning Environment (ALE) (Bellemare et al. 2013) games run at sixty frames per second, and agents can submit an action at every frame. Frame skip is the number of frames an action is repeated before a new action is selected. Existing reinforcement learning (RL) approaches use static frame skip: HNEAT (Hausknecht et al. 2013) uses a frame skip of 0; DQN (Mnih et al. 2013) uses a frame skip of 2-3; SARSA and planning approaches (Bellemare et al. 2013) use a frame skip of 4. When action selection is computationally intensive , setting a higher frame skip can significantly decrease the time it takes to simulate an episode, at the cost of missing opportunities that only exist at a finer resolution. A large frame skip can also prevent degenerate superhuman reflex strategies, such as those described by Hausknecht et al. for Bowling, Kung Fu Master, Video Pinball and Beam Rider. We show that in addition to these advantages agents that act with high frame skip can actually learn faster with respect to the number of training episodes than those that skip no frames. We present results for six of the seven games covered by Mnih et al.: three (Beam Rider, Breakout and Pong) for which DQN was able to achieve near-or superhuman performance, and three (Q*Bert, Space Invaders and Seaquest) for which all RL approaches are far from human performance. These latter games were understood to be difficult because they require 'strategy that extends over long time scales.},
author = {Braylan, Alex and Hollenbeck, Mark and Meyerson, Elliot and Miikkulainen, Risto},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Braylan et al. - 2015 - Frame Skip Is a Powerful Parameter for Learning to Play Atari.pdf:pdf},
keywords = {AAAI Technical Report WS-15-10},
title = {{Frame Skip Is a Powerful Parameter for Learning to Play Atari}},
url = {www.aaai.org},
year = {2015}
}
@inproceedings{Ramirez-Hernandez2007,
author = {Ramirez-Hernandez, Jose A. and Fernandez, Emmanuel},
booktitle = {2007 IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning},
doi = {10.1109/ADPRL.2007.368189},
isbn = {1-4244-0706-0},
month = {apr},
pages = {201--208},
publisher = {IEEE},
title = {{An Approximate Dynamic Programming Approach for Job Releasing and Sequencing in a Reentrant Manufacturing Line}},
url = {https://ieeexplore.ieee.org/document/4220834/},
year = {2007}
}
@inproceedings{Kakade2001b,
author = {Kakade, Sham},
booktitle = {International Conference on Computational Learning Theory 2001},
doi = {10.1007/3-540-44581-1_40},
pages = {605--615},
publisher = {Springer, Berlin, Heidelberg},
title = {{Optimizing Average Reward Using Discounted Rewards}},
url = {http://link.springer.com/10.1007/3-540-44581-1{\_}40},
year = {2001}
}
@inproceedings{Chang-chunLiu,
author = {{Chang-chun Liu} and {Hui-yu Jin} and {Yu Tian} and {Hai-bin Yu}},
booktitle = {2001 International Conferences on Info-Tech and Info-Net. Proceedings (Cat. No.01EX479)},
doi = {10.1109/ICII.2001.983070},
isbn = {0-7803-7010-4},
pages = {280--285},
publisher = {IEEE},
title = {{Reinforcement learning approach to re-entrant manufacturing system scheduling}},
url = {http://ieeexplore.ieee.org/document/983070/},
volume = {3},
year = {2001}
}
@article{Liu2012,
abstract = {In this paper, we are interested in investigating the perturbation bounds for the stationary distributions for discrete-time or continuous-time Markov chains on a countable state space. For discrete-time Markov chains, two new normwise bounds are obtained. The first bound is rather easy to obtain since the needed condition, equivalent to uniform ergodicity, is imposed on the transition matrix directly. The second bound, which holds for a general (possibly periodic) Markov chain, involves finding a drift function. This drift function is closely related to the mean first hitting times. Some {\$}V{\$}-normwise bounds are also derived based on the results in [N. V. Kartashov, J. Soviet Math., 34 (1986), pp. 1493--1498]. Moreover, we show how the bounds developed in this paper and one bound given in [E. Seneta, Adv. Appl. Probab., 20 (1988), pp. 228--230] can be extended to continuous-time Markov chains. Several examples are shown to illustrate our results or to compare our bounds with the known ones in the literature.},
author = {Liu, Yuanyuan},
doi = {10.1137/110838753},
issn = {0895-4798},
journal = {SIAM Journal on Matrix Analysis and Applications},
keywords = {15B51,60J10,60J27,Markov chains,mean first hitting times,perturbation theory,stationary distribution,uniform ergodicity},
month = {jan},
number = {4},
pages = {1057--1074},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Perturbation Bounds for the Stationary Distributions of Markov Chains}},
url = {http://epubs.siam.org/doi/10.1137/110838753},
volume = {33},
year = {2012}
}
@article{Sidford2017a,
abstract = {In this paper we provide faster algorithms for approximately solving discounted Markov Decision Processes in multiple parameter regimes. Given a discounted Markov Decision Process (DMDP) with {\$}|S|{\$} states, {\$}|A|{\$} actions, discount factor {\$}\backslashgamma\backslashin(0,1){\$}, and rewards in the range {\$}[-M, M]{\$}, we show how to compute an {\$}\backslashepsilon{\$}-optimal policy, with probability {\$}1 - \backslashdelta{\$} in time $\backslash$[ $\backslash$tilde{\{}O{\}}$\backslash$left( $\backslash$left(|S|{\^{}}2 |A| + $\backslash$frac{\{}|S| |A|{\}}{\{}(1 - $\backslash$gamma){\^{}}3{\}} $\backslash$right) $\backslash$log$\backslash$left( $\backslash$frac{\{}M{\}}{\{}$\backslash$epsilon{\}} $\backslash$right) $\backslash$log$\backslash$left( $\backslash$frac{\{}1{\}}{\{}$\backslash$delta{\}} $\backslash$right) $\backslash$right) {\~{}} . $\backslash$] This contribution reflects the first nearly linear time, nearly linearly convergent algorithm for solving DMDPs for intermediate values of {\$}\backslashgamma{\$}. We also show how to obtain improved sublinear time algorithms provided we can sample from the transition function in {\$}O(1){\$} time. Under this assumption we provide an algorithm which computes an {\$}\backslashepsilon{\$}-optimal policy with probability {\$}1 - \backslashdelta{\$} in time $\backslash$[ $\backslash$tilde{\{}O{\}} $\backslash$left($\backslash$frac{\{}|S| |A| M{\^{}}2{\}}{\{}(1 - $\backslash$gamma){\^{}}4 $\backslash$epsilon{\^{}}2{\}} $\backslash$log $\backslash$left($\backslash$frac{\{}1{\}}{\{}$\backslash$delta{\}}$\backslash$right) $\backslash$right) {\~{}}. $\backslash$] Lastly, we extend both these algorithms to solve finite horizon MDPs. Our algorithms improve upon the previous best for approximately computing optimal policies for fixed-horizon MDPs in multiple parameter regimes. Interestingly, we obtain our results by a careful modification of approximate value iteration. We show how to combine classic approximate value iteration analysis with new techniques in variance reduction. Our fastest algorithms leverage further insights to ensure that our algorithms make monotonic progress towards the optimal value. This paper is one of few instances in using sampling to obtain a linearly convergent linear programming algorithm and we hope that the analysis may be useful more broadly.},
archivePrefix = {arXiv},
arxivId = {1710.09988},
author = {Sidford, Aaron and Wang, Mengdi and Wu, Xian and Ye, Yinyu},
eprint = {1710.09988},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sidford et al. - 2017 - Variance Reduced Value Iteration and Faster Algorithms for Solving Markov Decision Processes.pdf:pdf},
month = {oct},
title = {{Variance Reduced Value Iteration and Faster Algorithms for Solving Markov Decision Processes}},
url = {http://arxiv.org/abs/1710.09988},
year = {2017}
}
@article{Bhonker2016,
abstract = {Mastering a video game requires skill, tactics and strategy. While these attributes may be acquired naturally by human players, teaching them to a computer program is a far more challenging task. In recent years, extensive research was carried out in the field of reinforcement learning and numerous algorithms were introduced, aiming to learn how to perform human tasks such as playing video games. As a result, the Arcade Learning Environment (ALE) (Bellemare et al., 2013) has become a commonly used benchmark environment allowing algorithms to train on various Atari 2600 games. In many games the state-of-the-art algorithms outperform humans. In this paper we introduce a new learning environment, the Retro Learning Environment --- RLE, that can run games on the Super Nintendo Entertainment System (SNES), Sega Genesis and several other gaming consoles. The environment is expandable, allowing for more video games and consoles to be easily added to the environment, while maintaining the same interface as ALE. Moreover, RLE is compatible with Python and Torch. SNES games pose a significant challenge to current algorithms due to their higher level of complexity and versatility.},
archivePrefix = {arXiv},
arxivId = {1611.02205},
author = {Bhonker, Nadav and Rozenberg, Shai and Hubara, Itay},
eprint = {1611.02205},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bhonker, Rozenberg, Hubara - 2016 - Playing SNES in the Retro Learning Environment.pdf:pdf;:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bhonker, Rozenberg, Hubara - 2016 - Playing SNES in the Retro Learning Environment(2).pdf:pdf},
month = {nov},
title = {{Playing SNES in the Retro Learning Environment}},
url = {http://arxiv.org/abs/1611.02205},
year = {2016}
}
@incollection{Harrison1988,
author = {Harrison, J. Michael},
doi = {10.1007/978-1-4613-8762-6_11},
pages = {147--186},
publisher = {Springer, New York, NY},
title = {{Brownian Models of Queueing Networks with Heterogeneous Customer Populations}},
url = {http://link.springer.com/10.1007/978-1-4613-8762-6{\_}11},
year = {1988}
}
@inproceedings{Kennedy1995,
author = {Kennedy, J. and Eberhart, R.},
booktitle = {Proceedings of ICNN'95 - International Conference on Neural Networks},
doi = {10.1109/ICNN.1995.488968},
isbn = {0-7803-2768-3},
pages = {1942--1948},
publisher = {IEEE},
title = {{Particle swarm optimization}},
url = {http://ieeexplore.ieee.org/document/488968/},
volume = {4},
year = {1995}
}
@article{Wilson2014,
author = {Wilson, Aaron and Fern, Alan and Tadepalli, Prasad},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wilson, Fern, Tadepalli - 2014 - Using Trajectory Data to Improve Bayesian Optimization for Reinforcement Learning.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {Wilson2014},
pages = {253--282},
title = {{Using Trajectory Data to Improve Bayesian Optimization for Reinforcement Learning}},
url = {http://jmlr.org/papers/v15/wilson14a.html},
volume = {15},
year = {2014}
}
@inproceedings{Algorta2017,
abstract = {The game of Tetris is an important benchmark for research in artificial intelligence and machine learning. This paper provides a historical account of the algorithmic developments in Tetris and discusses open challenges. Hand-crafted controllers, genetic algorithms, and reinforcement learning have all contributed to good solutions. However, existing solutions fall far short of what can be achieved by expert players playing without time pressure. Further study of the game has the potential to contribute to important areas of research, including feature discovery , autonomous learning of action hierarchies, and sample-efficient reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1905.01652v2},
author = {Algorta, Simon and Simsek, Ozgur},
booktitle = {34th International Conference on Machine Learning},
eprint = {1905.01652v2},
isbn = {1905.01652v2},
title = {{The Game of Tetris in Machine Learning}},
url = {https://arxiv.org/pdf/1905.01652.pdf},
year = {2017}
}
@article{Maglaras2000,
author = {Maglaras, Constantinos},
doi = {10.1214/aoap/1019487513},
issn = {1050-5164},
journal = {The Annals of Applied Probability},
keywords = {Queueing networks,asymptotic optimality,discrete-review policies,fluid models,trajectory tracking},
month = {aug},
number = {3},
pages = {897--929},
publisher = {Institute of Mathematical Statistics},
title = {{Discrete-review policies for scheduling stochastic networks: trajectory tracking and fluid-scale asymptotic optimality}},
url = {http://projecteuclid.org/euclid.aoap/1019487513},
volume = {10},
year = {2000}
}
@article{Bauerle2001a,
author = {Bauerle, Nicole},
doi = {10.1214/aoap/1019487606},
issn = {1050-5164},
journal = {The Annals of Applied Probability},
keywords = {Markov decision process,Stochastic network,fluid model,stochastic orderings,weak convergence},
month = {nov},
number = {4},
pages = {1065--1083},
publisher = {Institute of Mathematical Statistics},
title = {{Asymptotic optimality of tracking policies in stochastic networks}},
url = {http://projecteuclid.org/euclid.aoap/1019487606},
volume = {10},
year = {2001}
}
@article{DeFarias2003a,
abstract = {The curse of dimensionality gives rise to prohibitive computational requirements that render infeasible the exact solution of large-scale stochastic control problems. We study an efficient method b...},
author = {de Farias, D. P. and {Van Roy}, B.},
doi = {10.1287/opre.51.6.850.24925},
journal = {Operations Research},
number = {6},
pages = {850--865},
publisher = {INFORMS},
title = {{The Linear Programming Approach to Approximate Dynamic Programming}},
url = {http://pubsonline.informs.org/doi/abs/10.1287/opre.51.6.850.24925},
volume = {51},
year = {2003}
}
@article{Kingma2017,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy},
eprint = {1412.6980},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kingma, Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:pdf},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2017}
}
@inproceedings{Duan2016,
author = {Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning},
pages = {1329--1338},
publisher = {JMLR.org},
title = {{Benchmarking deep reinforcement learning for continuous control}},
url = {https://dl.acm.org/citation.cfm?id=3045531},
year = {2016}
}
@misc{DeepMind2016a,
author = {Gao, J. and Evans, R.},
title = {{DeepMind AI Reduces Google Data Centre Cooling Bill by 40{\%}}},
url = {https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/},
urldate = {2019-05-29},
year = {2016}
}
@article{Nichol2018a,
abstract = {In this report, we present a new reinforcement learning (RL) benchmark based on the Sonic the Hedgehog (TM) video game franchise. This benchmark is intended to measure the performance of transfer learning and few-shot learning algorithms in the RL domain. We also present and evaluate some baseline algorithms on the new benchmark.},
archivePrefix = {arXiv},
arxivId = {1804.03720},
author = {Nichol, Alex and Pfau, Vicki and Hesse, Christopher and Klimov, Oleg and Schulman, John},
eprint = {1804.03720},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nichol et al. - 2018 - Gotta Learn Fast A New Benchmark for Generalization in RL(2).pdf:pdf},
month = {apr},
title = {{Gotta Learn Fast: A New Benchmark for Generalization in RL}},
url = {http://arxiv.org/abs/1804.03720},
year = {2018}
}
@article{Sidford2017,
abstract = {In this paper we provide faster algorithms for approximately solving discounted Markov Decision Processes in multiple parameter regimes. Given a discounted Markov Decision Process (DMDP) with {\$}|S|{\$} states, {\$}|A|{\$} actions, discount factor {\$}\backslashgamma\backslashin(0,1){\$}, and rewards in the range {\$}[-M, M]{\$}, we show how to compute an {\$}\backslashepsilon{\$}-optimal policy, with probability {\$}1 - \backslashdelta{\$} in time $\backslash$[ $\backslash$tilde{\{}O{\}}$\backslash$left( $\backslash$left(|S|{\^{}}2 |A| + $\backslash$frac{\{}|S| |A|{\}}{\{}(1 - $\backslash$gamma){\^{}}3{\}} $\backslash$right) $\backslash$log$\backslash$left( $\backslash$frac{\{}M{\}}{\{}$\backslash$epsilon{\}} $\backslash$right) $\backslash$log$\backslash$left( $\backslash$frac{\{}1{\}}{\{}$\backslash$delta{\}} $\backslash$right) $\backslash$right) {\~{}} . $\backslash$] This contribution reflects the first nearly linear time, nearly linearly convergent algorithm for solving DMDPs for intermediate values of {\$}\backslashgamma{\$}. We also show how to obtain improved sublinear time algorithms provided we can sample from the transition function in {\$}O(1){\$} time. Under this assumption we provide an algorithm which computes an {\$}\backslashepsilon{\$}-optimal policy with probability {\$}1 - \backslashdelta{\$} in time $\backslash$[ $\backslash$tilde{\{}O{\}} $\backslash$left($\backslash$frac{\{}|S| |A| M{\^{}}2{\}}{\{}(1 - $\backslash$gamma){\^{}}4 $\backslash$epsilon{\^{}}2{\}} $\backslash$log $\backslash$left($\backslash$frac{\{}1{\}}{\{}$\backslash$delta{\}}$\backslash$right) $\backslash$right) {\~{}}. $\backslash$] Lastly, we extend both these algorithms to solve finite horizon MDPs. Our algorithms improve upon the previous best for approximately computing optimal policies for fixed-horizon MDPs in multiple parameter regimes. Interestingly, we obtain our results by a careful modification of approximate value iteration. We show how to combine classic approximate value iteration analysis with new techniques in variance reduction. Our fastest algorithms leverage further insights to ensure that our algorithms make monotonic progress towards the optimal value. This paper is one of few instances in using sampling to obtain a linearly convergent linear programming algorithm and we hope that the analysis may be useful more broadly.},
archivePrefix = {arXiv},
arxivId = {1710.09988},
author = {Sidford, Aaron and Wang, Mengdi and Wu, Xian and Ye, Yinyu},
eprint = {1710.09988},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sidford et al. - 2017 - Variance Reduced Value Iteration and Faster Algorithms for Solving Markov Decision Processes.pdf:pdf},
month = {oct},
title = {{Variance Reduced Value Iteration and Faster Algorithms for Solving Markov Decision Processes}},
url = {http://arxiv.org/abs/1710.09988},
year = {2017}
}
@inproceedings{Thomas,
abstract = {We show that several popular discounted reward natural actor-critics, including the popular NAC-LSTD and eNAC algorithms, do not generate un-biased estimates of the natural policy gradient as claimed. We derive the first unbiased discounted reward natural actor-critics using batch and iterative approaches to gradient estimation. We argue that the bias makes the existing algorithms more appropriate for the average reward setting. We also show that, when Sarsa($\lambda$) is guaranteed to converge to an optimal policy, the objective function used by natural actor-critics has only global optima, so policy gradient methods are guaranteed to converge to globally optimal policies as well.},
author = {Thomas, Philip S},
booktitle = {Proceedings of the 31 st International Conference on Machine Learning},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thomas - Unknown - Bias in Natural Actor-Critic Algorithms.pdf:pdf},
title = {{Bias in Natural Actor-Critic Algorithms}},
url = {http://proceedings.mlr.press/v32/thomas14.pdf},
year = {2014}
}
@article{Bauerle2001,
author = {B{\"{a}}uerle, Nicole},
doi = {10.1214/aoap/1019487606},
issn = {1050-5164},
journal = {The Annals of Applied Probability},
keywords = {Markov decision process,Stochastic network,fluid model,stochastic orderings,weak convergence},
month = {nov},
number = {4},
pages = {1065--1083},
publisher = {Institute of Mathematical Statistics},
title = {{Asymptotic optimality of tracking policies in stochastic networks}},
url = {http://projecteuclid.org/euclid.aoap/1019487606},
volume = {10},
year = {2001}
}
@article{Pohlen2018,
abstract = {Despite significant advances in the field of deep Reinforcement Learning (RL), today's algorithms still fail to learn human-level policies consistently over a set of diverse tasks such as Atari 2600 games. We identify three key challenges that any algorithm needs to master in order to perform well on all games: processing diverse reward distributions, reasoning over long time horizons, and exploring efficiently. In this paper, we propose an algorithm that addresses each of these challenges and is able to learn human-level policies on nearly all Atari games. A new transformed Bellman operator allows our algorithm to process rewards of varying densities and scales; an auxiliary temporal consistency loss allows us to train stably using a discount factor of {\$}\backslashgamma = 0.999{\$} (instead of {\$}\backslashgamma = 0.99{\$}) extending the effective planning horizon by an order of magnitude; and we ease the exploration problem by using human demonstrations that guide the agent towards rewarding states. When tested on a set of 42 Atari games, our algorithm exceeds the performance of an average human on 40 games using a common set of hyper parameters. Furthermore, it is the first deep RL algorithm to solve the first level of Montezuma's Revenge.},
archivePrefix = {arXiv},
arxivId = {1805.11593},
author = {Pohlen, Tobias and Piot, Bilal and Hester, Todd and Azar, Mohammad Gheshlaghi and Horgan, Dan and Budden, David and Barth-Maron, Gabriel and van Hasselt, Hado and Quan, John and Ve{\v{c}}er{\'{i}}k, Mel and Hessel, Matteo and Munos, R{\'{e}}mi and Pietquin, Olivier},
eprint = {1805.11593},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pohlen et al. - 2018 - Observe and Look Further Achieving Consistent Performance on Atari.pdf:pdf},
month = {may},
title = {{Observe and Look Further: Achieving Consistent Performance on Atari}},
url = {http://arxiv.org/abs/1805.11593},
year = {2018}
}
@article{Achiam2017,
abstract = {For many applications of reinforcement learning it can be more convenient to specify both a reward function and constraints, rather than trying to design behavior through the reward function. For example, systems that physically interact with or around humans should satisfy safety constraints. Recent advances in policy search algorithms (Mnih et al., 2016, Schulman et al., 2015, Lillicrap et al., 2016, Levine et al., 2016) have enabled new capabilities in high-dimensional control, but do not consider the constrained setting. We propose Constrained Policy Optimization (CPO), the first general-purpose policy search algorithm for constrained reinforcement learning with guarantees for near-constraint satisfaction at each iteration. Our method allows us to train neural network policies for high-dimensional control while making guarantees about policy behavior all throughout training. Our guarantees are based on a new theoretical result, which is of independent interest: we prove a bound relating the expected returns of two policies to an average divergence between them. We demonstrate the effectiveness of our approach on simulated robot locomotion tasks where the agent must satisfy constraints motivated by safety.},
archivePrefix = {arXiv},
arxivId = {1705.10528},
author = {Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter},
eprint = {1705.10528},
file = {:C$\backslash$:/Users/markgluzman/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Achiam et al. - 2017 - Constrained Policy Optimization(2).pdf:pdf},
month = {may},
title = {{Constrained Policy Optimization}},
url = {http://arxiv.org/abs/1705.10528},
year = {2017}
}
@inproceedings{Kim2017,
author = {Kim, Namyong and Shin, Hayong},
booktitle = {2017 Winter Simulation Conference (WSC)},
doi = {10.1109/WSC.2017.8248209},
isbn = {978-1-5386-3428-8},
month = {dec},
pages = {4570--4571},
publisher = {IEEE},
title = {{The application of actor-critic reinforcement learning for fab dispatching scheduling}},
url = {http://ieeexplore.ieee.org/document/8248209/},
year = {2017}
}
